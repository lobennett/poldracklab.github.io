[
  {
    "objectID": "lab_volunteers.html",
    "href": "lab_volunteers.html",
    "title": "Lab volunteers",
    "section": "",
    "text": "If you are a Stanford undergraduate interested in volunteering as a research assistant in the Poldrack Lab, you can sign up here.  Please note that we do not accept minors as volunteers in the laboratory."
  },
  {
    "objectID": "positions.html",
    "href": "positions.html",
    "title": "Open Positions",
    "section": "",
    "text": "The Poldrack lab is always looking for talented graduate students and postdoctoral researchers interested in our core research topics. See our current list of research projects to learn more about the lab’s ongoing work, and the lab mentoring policies to learn more about the graduate programs that the laboratory is affiliated with and Dr. Poldrack’s mentoring philosophy.",
    "crumbs": [
      "Getting involved",
      "Open Positions"
    ]
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Creating config file at /home/runner/.config/pybliometrics.cfg with default paths…",
    "crumbs": [
      "Publications"
    ]
  },
  {
    "objectID": "volunteer.html",
    "href": "volunteer.html",
    "title": "Research subjects",
    "section": "",
    "text": "Please note, these project are active as of August 2023. If you take our screener, we’ll be sure to contact you as soon as possible if you’re eligible.\n\n\nWe currently have the following opportunities available for research participants:\n\n\nJoin us as a participant!\n\nThe Poldrack Lab is recruiting volunteers for studies of how cognitive control, memory, and decision-making work in the brain. These studies involve up to 20 experimental sessions lasting up to three hours each. During each session, you will be scanned using functional magnetic resonance imaging and/or perform computerized or paper and pencil behavioral tasks and questionnaires. You may also be asked to respond to daily prompts on an app regarding your menstrual cycle and your sleep schedule. If you do not have a menstrual cycle, you may respond to only the prompts about your sleep schedule. After the scanning session, you may be asked to provide a saliva sample. You may be excluded for irregular menstrual cycles.\nEligible participants must have a minimum of an 8th grade education, speak English fluently, be right-handed, and have no history of significant medical illnesses including cardiovascular disease, cancer, immunodeficiency disorders (such as HIV infection), diabetes, unstable endocrine disorders, neurological disorders, neuromuscular disorders, or blood dyscrasias; additionally, participants must not have experienced head trauma with loss of consciousness, cerebrovascular accident, seizures, or neurosurgical interventions, and should have no history of major psychotic disorders (including schizophrenia and bipolar disorder), substance dependence, the use of medication for psychiatric reasons, or a clinical diagnosis of major depression.\nEligible participants will be compensated up to $20/hour for participation, and may have the opportunity to earn bonuses up to $400. If asked to respond to our cell-phone based app prompts on your sleep and menstrual cycles, participants will be paid $1 per day for answering the 3 daily questions. Participants may receive an additional bonus up to $100 for completing a high proportion of these daily questions. - All participants who are US citizens must provide a social security number to receive direct payment. - Participants who are not US citizens, but have a social security number, and a Green Card (Alien Registration Card) can be paid directly. Photocopy of Green Card is required. - Non-US citizens, without a Green Card (Alien Registration Card), must have a F-1, J-1 or H-1B Stanford sponsored visa, an I-94 card, a passport, and must also complete a LA-6 form (provided at time of participation). Copies of all paperwork will be submitted to Stanford University and the participant will receive payment from the University after appropriate taxes are withheld. NON-US CITIZENS WITHOUT A GREEN CARD CANNOT BE PAID AT THE TIME OF THE STUDY.\nFor complaints, concerns, or participant’s right questions, please contact the Stanford Institutional Review Board (IRB) at (650)-723-2480.\nIf you are interested, please fill out the recruitment form: https://redcap.stanford.edu/surveys/?s=LKFF39MRM7HRK4PC.",
    "crumbs": [
      "Getting involved",
      "Research subjects"
    ]
  },
  {
    "objectID": "labguide/funding/funding.html",
    "href": "labguide/funding/funding.html",
    "title": "Trainee Funding",
    "section": "",
    "text": "Most postdocs in the lab are funded through postdoctoral fellowships or grants that they write themselves.\n\nSome postdocs are funded through existing grants. In this case, the postdoc must spend a significant portion of their time working on that project. However, the lab also provides postdocs the room to develop their own ideas.\nWe have the opportunity to request Diversity Supplements for our NIH grants. If you qualify for such a fellowship (which includes individuals from underrepresented racial/ethnic groups, disadvantaged backgrounds, or rural/low-income areas), please contact Dr. Poldrack to discuss your interest.\n\nPostdocs should pay close attention to the details of their funding arrangement. For example, F32 NIH funding to the university comes with a financial account where the institutional allowance is included. Stanford taps this allowance for ‘university insurance’, so one should be aware that funds will deplete faster than may be presumed.",
    "crumbs": [
      "Lab guide",
      "Trainee Funding"
    ]
  },
  {
    "objectID": "labguide/funding/funding.html#how-are-postdocs-in-the-lab-funded",
    "href": "labguide/funding/funding.html#how-are-postdocs-in-the-lab-funded",
    "title": "Trainee Funding",
    "section": "",
    "text": "Most postdocs in the lab are funded through postdoctoral fellowships or grants that they write themselves.\n\nSome postdocs are funded through existing grants. In this case, the postdoc must spend a significant portion of their time working on that project. However, the lab also provides postdocs the room to develop their own ideas.\nWe have the opportunity to request Diversity Supplements for our NIH grants. If you qualify for such a fellowship (which includes individuals from underrepresented racial/ethnic groups, disadvantaged backgrounds, or rural/low-income areas), please contact Dr. Poldrack to discuss your interest.\n\nPostdocs should pay close attention to the details of their funding arrangement. For example, F32 NIH funding to the university comes with a financial account where the institutional allowance is included. Stanford taps this allowance for ‘university insurance’, so one should be aware that funds will deplete faster than may be presumed.",
    "crumbs": [
      "Lab guide",
      "Trainee Funding"
    ]
  },
  {
    "objectID": "labguide/mission_statement.html",
    "href": "labguide/mission_statement.html",
    "title": "Mission statement",
    "section": "",
    "text": "The fundamental mission of our lab is to understand how human brain computations give rise to decisions and actions, with a particular focus on how humans exert control over their behavior. We use a broad set of methods, including neuroimaging, behavior, and computation. We are committed to doing our work in as transparent and reproducible way as possible, and in building tools and resources to help other researchers achieve these aims as well. We aim to sustain a laboratory community in which everyone feels welcomed, respected, and intellectually stimulated.",
    "crumbs": [
      "Lab guide",
      "Mission statement"
    ]
  },
  {
    "objectID": "labguide/environment/expectations.html",
    "href": "labguide/environment/expectations.html",
    "title": "Expectations of lab engagement",
    "section": "",
    "text": "All researchers in the lab are expected to attend the weekly lab meeting, except in cases of unavoidable conflict (such as course scheduling). It generally lasts 1-2 hours and all the members are invited to share the main activities of the last week. Additional activities at the lab meeting include: presentations of research projects or data; talks by external speakers; discussions of specific research papers; code reviews in which a lab member’s code is reviewed online with a focus on refactoring.\n\nAll members of the information technology team are expected to attend the weekly IT meeting.\nDr. Poldrack relies primarily upon email for communication, and generally responds quickly to emails unless traveling or away from work. He expects lab members to respond to emails within one business day unless they are away from work (even if the response is simply to acknowledge receipt of the message).",
    "crumbs": [
      "Lab guide",
      "Lab environment",
      "Expectations of lab engagement"
    ]
  },
  {
    "objectID": "labguide/environment/code_of_conduct.html",
    "href": "labguide/environment/code_of_conduct.html",
    "title": "Code of conduct",
    "section": "",
    "text": "All lab members and guests are expected to adhere to the following code of conduct. In addition, all Stanford University affiliates are required to adhere to the University Code of Conduct. For definitions of an act of intolerance & hate crime escalation policy please see this page on University AOI. If you wish to speak with someone outside of the lab regarding an incident that occurred in or outside of the lab, the university has this list of resources.",
    "crumbs": [
      "Lab guide",
      "Lab environment",
      "Code of conduct"
    ]
  },
  {
    "objectID": "labguide/environment/code_of_conduct.html#statement-of-intent",
    "href": "labguide/environment/code_of_conduct.html#statement-of-intent",
    "title": "Code of conduct",
    "section": "Statement of intent",
    "text": "Statement of intent\nOur lab is dedicated to providing a safe and welcoming environment for everyone. We do not tolerate harassment for any form of identity such as but not limited to: sex, gender identity, age, sexual orientation, disability including mental illness, physical appearance, body size, race, ethnicity, religion (including the lack thereof), or socioeconomic status. We do not tolerate harassment of lab members or guests in any form. Sexual language and imagery is not appropriate for any lab event or venue, including talks, workshops, parties, Twitter and other online media.",
    "crumbs": [
      "Lab guide",
      "Lab environment",
      "Code of conduct"
    ]
  },
  {
    "objectID": "labguide/environment/code_of_conduct.html#details-of-lab-code-of-conduct",
    "href": "labguide/environment/code_of_conduct.html#details-of-lab-code-of-conduct",
    "title": "Code of conduct",
    "section": "Details of Lab Code of Conduct",
    "text": "Details of Lab Code of Conduct\n\nHarassment includes offensive verbal comments related to a person’s identity or presentation of their identity. This can include denigrating comments, microaggressions, or consistent interruption during talks or presentations\nSexual harassment includes: displays of sexual images in public lab spaces, deliberate intimidation, stalking/ following, coerced photography or recording, inappropriate physical contact, and unwelcome sexual attention.",
    "crumbs": [
      "Lab guide",
      "Lab environment",
      "Code of conduct"
    ]
  },
  {
    "objectID": "labguide/environment/code_of_conduct.html#responding-to-incidents",
    "href": "labguide/environment/code_of_conduct.html#responding-to-incidents",
    "title": "Code of conduct",
    "section": "Responding to incidents",
    "text": "Responding to incidents\n\nLab members asked to stop any misconduct are expected to comply immediately. Justification for the request does not need to be provided, nor does the reporting party of the incident need to be identified.\nIf you are being harassed, notice that someone else is being harassed, or have any other concerns, please contact Dr. Poldrack as soon as possible. Anonymous comments may also be made to Dr. Poldrack using this form. If you are not comfortable contacting him directly, you may report your concerns to one of the relevant Stanford University offices that are prepared to respond to harassment complaints.\nWe expect lab members to follow this code of conduct at conferences and workshops and related social events, and whenever they are representing the lab in a professional context.\nWhen lab members are invited to participate in a professional event, they are expected to inquire regarding the steps that the organizers are taking to ensure diversity prior to accepting any invitation. This includes declining participation in exclusionary events such as panels or talks. For example, men in the lab should avoid participating in all-men panels whenever possible, keeping in mind that this may be more achievable for senior versus junior lab members.\n\nAdapted from https://en-us.confcodeofconduct.com/.",
    "crumbs": [
      "Lab guide",
      "Lab environment",
      "Code of conduct"
    ]
  },
  {
    "objectID": "labguide/environment/survey.html",
    "href": "labguide/environment/survey.html",
    "title": "Annual environment survey",
    "section": "",
    "text": "In order to ensure that the environment remains welcoming and productive for everyone, we will conduct a yearly survey of lab members regarding the lab environment (link to example). This survey will be completed at the end of the Spring Quarter (in June).",
    "crumbs": [
      "Lab guide",
      "Lab environment",
      "Annual environment survey"
    ]
  },
  {
    "objectID": "labguide/research/data_management.html",
    "href": "labguide/research/data_management.html",
    "title": "Data management and sharing",
    "section": "",
    "text": "All research-related materials (including data and code) should be stored on a volume that is automatically backed up to a cloud storage server, such as Google Drive (which provides free storage via your Stanford login).\n\nStanford also provides access to Crashplan backups for faculty and staff which automatically backs up laptops and computers.\n\n\nNeuroimaging datasets should be backed up on TACC Corral.\n\nEvery dataset should be organized with the goal that another researcher could take the dataset and immediately understand its content without the need to ask questions of the dataset owner.\nAll imaging data should be organized using the BIDS format.\nFor non-imaging data, files and folders should be named and organized using the Psych-DS format when possible.\nVariable names should be as expressive as possible, with automated parsing in mind. The structure of variable names should follow the key-value schema used in BIDS (with key and value separated by a dash, and key-value pairs separated by underscores). All embedded numbers should be zero-padded.\n\nExamples:\nfor item 5 on the Barratt Impulsiveness Scale, the variable name might be “survey-BIS_item-005”\n\nAll datasets should be accompanied by a data dictionary that specifies the meaning of each variable.\n\nThis should preferably be stored as a JSON file, with variable names as dictionary keys to support automated parsing.\n\n\n\n\n\n\nAll data generated within our lab will be shared upon publication at the latest, and preferably upon preprint submission.\nIn some cases, we may work with datasets that we are not able to share in full, due to data use agreements or legal restrictions. In this case, we will push to share at least the processed data necessary to run the primary statistical analyses.\nData will be shared through platforms that allow snapshotting and generation of a DOI:\n\nOpenNeuro for neuroimaging data\nFor behavioral data, repositories include Zenodo (preferred), OSF, Stanford Digital Repository, or Dryad.\nLarge data files should not be committed to Github repositories. Small data files may be hosted on github, but it should not be the primary sharing platform.\n\n\nA snapshot should be shared that matches exactly the analyses in the publication, and which can be analyzed directly using the shared analysis code.\nAll data will be shared with an explicit data use agreement (aka “license”)\n\nWe prefer the minimally restrictive license possible, preferably CC0\n\nAll data should be shared with an explicit description of how they should be cited.\nWhen using shared data, lab members should ensure that they properly cite the data source in any publications using the language recommended by the data owner.",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Data management and sharing"
    ]
  },
  {
    "objectID": "labguide/research/data_management.html#data-management",
    "href": "labguide/research/data_management.html#data-management",
    "title": "Data management and sharing",
    "section": "",
    "text": "All research-related materials (including data and code) should be stored on a volume that is automatically backed up to a cloud storage server, such as Google Drive (which provides free storage via your Stanford login).\n\nStanford also provides access to Crashplan backups for faculty and staff which automatically backs up laptops and computers.\n\n\nNeuroimaging datasets should be backed up on TACC Corral.\n\nEvery dataset should be organized with the goal that another researcher could take the dataset and immediately understand its content without the need to ask questions of the dataset owner.\nAll imaging data should be organized using the BIDS format.\nFor non-imaging data, files and folders should be named and organized using the Psych-DS format when possible.\nVariable names should be as expressive as possible, with automated parsing in mind. The structure of variable names should follow the key-value schema used in BIDS (with key and value separated by a dash, and key-value pairs separated by underscores). All embedded numbers should be zero-padded.\n\nExamples:\nfor item 5 on the Barratt Impulsiveness Scale, the variable name might be “survey-BIS_item-005”\n\nAll datasets should be accompanied by a data dictionary that specifies the meaning of each variable.\n\nThis should preferably be stored as a JSON file, with variable names as dictionary keys to support automated parsing.",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Data management and sharing"
    ]
  },
  {
    "objectID": "labguide/research/data_management.html#data-sharing",
    "href": "labguide/research/data_management.html#data-sharing",
    "title": "Data management and sharing",
    "section": "",
    "text": "All data generated within our lab will be shared upon publication at the latest, and preferably upon preprint submission.\nIn some cases, we may work with datasets that we are not able to share in full, due to data use agreements or legal restrictions. In this case, we will push to share at least the processed data necessary to run the primary statistical analyses.\nData will be shared through platforms that allow snapshotting and generation of a DOI:\n\nOpenNeuro for neuroimaging data\nFor behavioral data, repositories include Zenodo (preferred), OSF, Stanford Digital Repository, or Dryad.\nLarge data files should not be committed to Github repositories. Small data files may be hosted on github, but it should not be the primary sharing platform.\n\n\nA snapshot should be shared that matches exactly the analyses in the publication, and which can be analyzed directly using the shared analysis code.\nAll data will be shared with an explicit data use agreement (aka “license”)\n\nWe prefer the minimally restrictive license possible, preferably CC0\n\nAll data should be shared with an explicit description of how they should be cited.\nWhen using shared data, lab members should ensure that they properly cite the data source in any publications using the language recommended by the data owner.",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Data management and sharing"
    ]
  },
  {
    "objectID": "labguide/research/data_analysis_procedures.html",
    "href": "labguide/research/data_analysis_procedures.html",
    "title": "Data analysis procedures",
    "section": "",
    "text": "We affirm the scientific utility and importance of both exploratory and hypothesis-driven (confirmatory) research, but it is essential that they be clearly distinguished.\nAll research will be considered exploratory unless the hypotheses and analysis methods were pre-registered.\n\n\n\n\n\nFor datasets where it is feasible, the data should be visually examined for outliers or aberrant patterns (e.g. zero-inflation) using pair plots or other appropriate methods.\nAnalysis procedures should be tested on the first subject (for fMRI) or first few subjects (for behavioral studies) before collecting any additional data, to ensure that the data are appropriately recorded.\nFor MRI data being acquired by the lab, research coordinators should run MRIQC and view the reports within one week of acquisition.\n\n\n\n\n\nWhen possible, all analysis code should be tested on simulated data to ensure positive and negative control prior to any analysis of the real data.\nPositive controls should ensure that effects of interest are detected when present\n\nThese can also be extended to perform power analysis.\n\nNegative controls should ensure that no effects are detected when the null is true\nIn some cases, it is necessary to understand the nature of the data (e.g. distributions of variables) in order to develop appropriate analysis and simulation code. In these cases, the researcher will ask Dr. Poldrack or another lab member to generate a simulated dataset based on their actual data (e.g. by randomly shuffling the observations of each variable to create a knockoff dataset that maintains each variable’s marginal distribution), prior to any analyses.\n\n\n\n\n\nWhen possible, all data analyses should be replicated by an independent analyst who is blind to the results of the initial analysis.\nIf discordant results are observed, then the analysts will work together to determine their source, and whether they reflect error versus true analytic variability.\n\n\n\n\n\nAll analysis procedures should be implemented in code and runnable with a single command.\nWe aim to avoid any analyses that involve manual intervention. If such analyses are necessary (e.g. for identifying anatomical brain regions), the specific protocol should be described, and if possible the manual operation should be recorded using screen capture.\nThe entire software environment should be at best reproducible, or at minimum recordable.\n\nThe best practice is to implement all analyses within a Docker image that can be shared with the code.\nIf possible, the versions of all software libraries should be pinned.\nAt minimum, the entire software environment (including library versions) should be recorded and included with the shared results.",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Data analysis procedures"
    ]
  },
  {
    "objectID": "labguide/research/data_analysis_procedures.html#exploratory-vs.-confirmatory-analyses",
    "href": "labguide/research/data_analysis_procedures.html#exploratory-vs.-confirmatory-analyses",
    "title": "Data analysis procedures",
    "section": "",
    "text": "We affirm the scientific utility and importance of both exploratory and hypothesis-driven (confirmatory) research, but it is essential that they be clearly distinguished.\nAll research will be considered exploratory unless the hypotheses and analysis methods were pre-registered.",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Data analysis procedures"
    ]
  },
  {
    "objectID": "labguide/research/data_analysis_procedures.html#data-quality-control",
    "href": "labguide/research/data_analysis_procedures.html#data-quality-control",
    "title": "Data analysis procedures",
    "section": "",
    "text": "For datasets where it is feasible, the data should be visually examined for outliers or aberrant patterns (e.g. zero-inflation) using pair plots or other appropriate methods.\nAnalysis procedures should be tested on the first subject (for fMRI) or first few subjects (for behavioral studies) before collecting any additional data, to ensure that the data are appropriately recorded.\nFor MRI data being acquired by the lab, research coordinators should run MRIQC and view the reports within one week of acquisition.",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Data analysis procedures"
    ]
  },
  {
    "objectID": "labguide/research/data_analysis_procedures.html#code-validation",
    "href": "labguide/research/data_analysis_procedures.html#code-validation",
    "title": "Data analysis procedures",
    "section": "",
    "text": "When possible, all analysis code should be tested on simulated data to ensure positive and negative control prior to any analysis of the real data.\nPositive controls should ensure that effects of interest are detected when present\n\nThese can also be extended to perform power analysis.\n\nNegative controls should ensure that no effects are detected when the null is true\nIn some cases, it is necessary to understand the nature of the data (e.g. distributions of variables) in order to develop appropriate analysis and simulation code. In these cases, the researcher will ask Dr. Poldrack or another lab member to generate a simulated dataset based on their actual data (e.g. by randomly shuffling the observations of each variable to create a knockoff dataset that maintains each variable’s marginal distribution), prior to any analyses.",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Data analysis procedures"
    ]
  },
  {
    "objectID": "labguide/research/data_analysis_procedures.html#multiple-analysts",
    "href": "labguide/research/data_analysis_procedures.html#multiple-analysts",
    "title": "Data analysis procedures",
    "section": "",
    "text": "When possible, all data analyses should be replicated by an independent analyst who is blind to the results of the initial analysis.\nIf discordant results are observed, then the analysts will work together to determine their source, and whether they reflect error versus true analytic variability.",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Data analysis procedures"
    ]
  },
  {
    "objectID": "labguide/research/data_analysis_procedures.html#reproducible-analysis",
    "href": "labguide/research/data_analysis_procedures.html#reproducible-analysis",
    "title": "Data analysis procedures",
    "section": "",
    "text": "All analysis procedures should be implemented in code and runnable with a single command.\nWe aim to avoid any analyses that involve manual intervention. If such analyses are necessary (e.g. for identifying anatomical brain regions), the specific protocol should be described, and if possible the manual operation should be recorded using screen capture.\nThe entire software environment should be at best reproducible, or at minimum recordable.\n\nThe best practice is to implement all analyses within a Docker image that can be shared with the code.\nIf possible, the versions of all software libraries should be pinned.\nAt minimum, the entire software environment (including library versions) should be recorded and included with the shared results.",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Data analysis procedures"
    ]
  },
  {
    "objectID": "labguide/research/coding_standards.html",
    "href": "labguide/research/coding_standards.html",
    "title": "Coding standards",
    "section": "",
    "text": "Coding standards\n\nCode should be readable\nAll lab members should be familiar with principles of readable coding:\n\nArt of Readable Code\nClean Code\n\nCode should be modular\n\nFunctions should do a single thing that is clearly expressed in the name of the function\nFunctions should include a docstring that clearly specifies input and output\n\nCode should be portable\n\nAny absolute paths should be specified as a variable in a single location, or preferably as a command line argument\nAny required environment variables should be clearly described\nAny non-standard requirements (e.g. Python libraries not available through PYPI) should be described with instructions on how to install\n\nImportant functions should be tested",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Coding standards"
    ]
  },
  {
    "objectID": "labguide/research/documentation.html",
    "href": "labguide/research/documentation.html",
    "title": "Documenting research activities",
    "section": "",
    "text": "Documenting research activities\n\nDetailed procedures should be documented throughout any ongoing research activities, including subject recruitment procedures, subject running procedures, analysis plans, and readme’s for any code base. Documentation should be sufficient to allow a new researcher or research coordinator to be able to understand any ongoing research procedures with minimal (ideally no) additional guidance from other research staff.\nThis documentation should be ready to be made available to Dr. Poldrack or other lab members at any time, preferably via an online platform such as Google Docs.",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Documenting research activities"
    ]
  },
  {
    "objectID": "labguide/research/statistical_methods.html",
    "href": "labguide/research/statistical_methods.html",
    "title": "Statistical methods",
    "section": "",
    "text": "Do not hesitate to ask Jeanette Mumford for help with any statistical analyses.\n\n\n\n\n\nMethods for multiple testing correction should be chosen prior to data analysis and reported in the preregistration document.\nThe nature of grouping for multiple testing corrections (e.g., parameter within single models, sets of models, etc) should be pre-specified.\nFor fMRI data, if using cluster-based thresholding the cluster forming threshold must be Z ≥ 3.1 if using parametric thresholding. There are no restrictions on threshold when using nonparametric thresholding, but the threshold should be pre-specified.\n\n\n\n\n\nProper cross-validation practices should be considered. Refer to https://pubmed.ncbi.nlm.nih.gov/31774490/ for more detail.\n\nIn-sample model fit indices should not be reported as evidence for predictive accuracy\nThe cross-validation procedure should encompass all operations applied to the data\nPrediction analyses should not be performed with samples smaller than several hundred observations\nMultiple measures of prediction accuracy should be examined and reported\nThe coefficient of determination should be computed using the sums of squares formulation and not the squared correlation coefficient\nLeave-one-out cross-validation should be avoided in favor of shuffle-split or K-fold cross-validation.\n\n\n\n\n\n\nMixed effects models should include all possible random effects, subject to the constraint of adequate model convergence.\nConvergence should be checked for all models, and models should be checked for highly influential observations.\nSimplification or random effects structure can be done if the p-value &gt; 0.2 (https://www.google.com/url?q=https://www.sciencedirect.com/science/article/pii/S0749596X17300013&sa=D&source=editors&ust=1623857125982000&usg=AOvVaw2E_twuab7e56Nn_i1yTdmn) assuming convergence.\n\n\n\n\n\nDoes the model properly test the hypothesis of interest?\nDo the data conform to the assumptions of the model?\n\nAll model fits should be criticized to identify violation of any assumptions or presence of outliers.\n\n\n\n\n\nEffect size, parameter estimate, standard error, p-value and confidence interval should all be reported.\nIn cases where null effects are reported, some measure of evidence for the null hypothesis (e.g. Bayes factors, equivalence tests) should be reported.\nResults for all analyses that were preregistered should be reported.",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Statistical methods"
    ]
  },
  {
    "objectID": "labguide/research/statistical_methods.html#need-help",
    "href": "labguide/research/statistical_methods.html#need-help",
    "title": "Statistical methods",
    "section": "",
    "text": "Do not hesitate to ask Jeanette Mumford for help with any statistical analyses.",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Statistical methods"
    ]
  },
  {
    "objectID": "labguide/research/statistical_methods.html#multiple-testing",
    "href": "labguide/research/statistical_methods.html#multiple-testing",
    "title": "Statistical methods",
    "section": "",
    "text": "Methods for multiple testing correction should be chosen prior to data analysis and reported in the preregistration document.\nThe nature of grouping for multiple testing corrections (e.g., parameter within single models, sets of models, etc) should be pre-specified.\nFor fMRI data, if using cluster-based thresholding the cluster forming threshold must be Z ≥ 3.1 if using parametric thresholding. There are no restrictions on threshold when using nonparametric thresholding, but the threshold should be pre-specified.",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Statistical methods"
    ]
  },
  {
    "objectID": "labguide/research/statistical_methods.html#cross-validation",
    "href": "labguide/research/statistical_methods.html#cross-validation",
    "title": "Statistical methods",
    "section": "",
    "text": "Proper cross-validation practices should be considered. Refer to https://pubmed.ncbi.nlm.nih.gov/31774490/ for more detail.\n\nIn-sample model fit indices should not be reported as evidence for predictive accuracy\nThe cross-validation procedure should encompass all operations applied to the data\nPrediction analyses should not be performed with samples smaller than several hundred observations\nMultiple measures of prediction accuracy should be examined and reported\nThe coefficient of determination should be computed using the sums of squares formulation and not the squared correlation coefficient\nLeave-one-out cross-validation should be avoided in favor of shuffle-split or K-fold cross-validation.",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Statistical methods"
    ]
  },
  {
    "objectID": "labguide/research/statistical_methods.html#mixed-effects-models",
    "href": "labguide/research/statistical_methods.html#mixed-effects-models",
    "title": "Statistical methods",
    "section": "",
    "text": "Mixed effects models should include all possible random effects, subject to the constraint of adequate model convergence.\nConvergence should be checked for all models, and models should be checked for highly influential observations.\nSimplification or random effects structure can be done if the p-value &gt; 0.2 (https://www.google.com/url?q=https://www.sciencedirect.com/science/article/pii/S0749596X17300013&sa=D&source=editors&ust=1623857125982000&usg=AOvVaw2E_twuab7e56Nn_i1yTdmn) assuming convergence.",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Statistical methods"
    ]
  },
  {
    "objectID": "labguide/research/statistical_methods.html#model-suitability-and-diagnostics",
    "href": "labguide/research/statistical_methods.html#model-suitability-and-diagnostics",
    "title": "Statistical methods",
    "section": "",
    "text": "Does the model properly test the hypothesis of interest?\nDo the data conform to the assumptions of the model?\n\nAll model fits should be criticized to identify violation of any assumptions or presence of outliers.",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Statistical methods"
    ]
  },
  {
    "objectID": "labguide/research/statistical_methods.html#reporting-results",
    "href": "labguide/research/statistical_methods.html#reporting-results",
    "title": "Statistical methods",
    "section": "",
    "text": "Effect size, parameter estimate, standard error, p-value and confidence interval should all be reported.\nIn cases where null effects are reported, some measure of evidence for the null hypothesis (e.g. Bayes factors, equivalence tests) should be reported.\nResults for all analyses that were preregistered should be reported.",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Statistical methods"
    ]
  },
  {
    "objectID": "labguide/computing/sherlock/data-management.html",
    "href": "labguide/computing/sherlock/data-management.html",
    "title": "The Poldrack Lab \\@Stanford",
    "section": "",
    "text": "This section describes how the lab manages datasets on Sherlock, including setting permissions (i.e., who else in the lab can access the dataset).\nDatasets that are considered to be common lab assets (which includes any new studies within the lab and any openly shared datasets) should be placed into the primary data directory on the relevant filesystem. Datasets that are in process of acquisition should go into the “inprocess” directory. Once the dataset is finalized, it should be moved into the “data” directory.\nOnce a dataset has been installed in the data directory, it should be changed to be read-only for owner and group, using the following commands:\nfind &lt;directory name&gt; -type d -exec chmod 550\nfind &lt;directory name&gt; -type f -exec chmod 440\nDatasets that are temporary, or files generated for analyses that are not intended to be reused or shared, should be placed within the user directory.\n\n\nSome data resources cannot be shared across the lab and instead need to be restricted to lab members with Data Usage Agreement (DUA) access. This can be done via access control lists (ACLs), which allow a resource to be owned by a particular owner/group, but applies an additional more specific set of permissions. The following can be adapted to restrict ACLs to an appropriate subset of lab members:\n\n\nprotect_access.sh\n\n#!/bin/bash\n\necho \"Using ACLs to restrict folder access on oak for russpold folders\"\necho -e \"\\t https://www.sherlock.stanford.edu/docs/storage/data-sharing/#posix-acls \"\nsleep 1\necho\n# get user input for directory + user\nread -p \"Enter the folder path: \" dir_name\nif [ ! -d \"$dir_name\" ]; then\n    echo \"Error: ${dir_name} doesn't exist\"\n    exit 1\nfi\n\nread -p \"Enter the username: \" user_name\n\n# set restrictions, repeating once for each desired user\necho -e \"Setting restrictions for ${user_name} as rxw for folder: /n ${dir_name}\"\nsetfacl -R -m u:$user_name:rwx $dir_name\nsetfacl -R -d -m u:$user_name:rwx $dir_name\n# repeat the above commands, replacing $user_name with the usernames\n# for additional members\n\n# rm default permissions for the group -- oak_russpold\nsetfacl -m d::group:oak_russpold:--- $dir_name",
    "crumbs": [
      "Lab guide",
      "Computing",
      "Sherlock",
      "Data management on Sherlock"
    ]
  },
  {
    "objectID": "labguide/computing/sherlock/data-management.html#data-management-on-sherlock",
    "href": "labguide/computing/sherlock/data-management.html#data-management-on-sherlock",
    "title": "The Poldrack Lab \\@Stanford",
    "section": "",
    "text": "This section describes how the lab manages datasets on Sherlock, including setting permissions (i.e., who else in the lab can access the dataset).\nDatasets that are considered to be common lab assets (which includes any new studies within the lab and any openly shared datasets) should be placed into the primary data directory on the relevant filesystem. Datasets that are in process of acquisition should go into the “inprocess” directory. Once the dataset is finalized, it should be moved into the “data” directory.\nOnce a dataset has been installed in the data directory, it should be changed to be read-only for owner and group, using the following commands:\nfind &lt;directory name&gt; -type d -exec chmod 550\nfind &lt;directory name&gt; -type f -exec chmod 440\nDatasets that are temporary, or files generated for analyses that are not intended to be reused or shared, should be placed within the user directory.\n\n\nSome data resources cannot be shared across the lab and instead need to be restricted to lab members with Data Usage Agreement (DUA) access. This can be done via access control lists (ACLs), which allow a resource to be owned by a particular owner/group, but applies an additional more specific set of permissions. The following can be adapted to restrict ACLs to an appropriate subset of lab members:\n\n\nprotect_access.sh\n\n#!/bin/bash\n\necho \"Using ACLs to restrict folder access on oak for russpold folders\"\necho -e \"\\t https://www.sherlock.stanford.edu/docs/storage/data-sharing/#posix-acls \"\nsleep 1\necho\n# get user input for directory + user\nread -p \"Enter the folder path: \" dir_name\nif [ ! -d \"$dir_name\" ]; then\n    echo \"Error: ${dir_name} doesn't exist\"\n    exit 1\nfi\n\nread -p \"Enter the username: \" user_name\n\n# set restrictions, repeating once for each desired user\necho -e \"Setting restrictions for ${user_name} as rxw for folder: /n ${dir_name}\"\nsetfacl -R -m u:$user_name:rwx $dir_name\nsetfacl -R -d -m u:$user_name:rwx $dir_name\n# repeat the above commands, replacing $user_name with the usernames\n# for additional members\n\n# rm default permissions for the group -- oak_russpold\nsetfacl -m d::group:oak_russpold:--- $dir_name",
    "crumbs": [
      "Lab guide",
      "Computing",
      "Sherlock",
      "Data management on Sherlock"
    ]
  },
  {
    "objectID": "labguide/computing/sherlock/access-and-resources.html",
    "href": "labguide/computing/sherlock/access-and-resources.html",
    "title": "The Poldrack Lab \\@Stanford",
    "section": "",
    "text": "This section describes getting initial access to Sherlock and monitoring available resources.\n\n\n\n\nIf you are a new member of a lab at stanford, you will need to have your PI email Sherlock’s support to get your SUNet account configured for use with computing resources. See the Sherlock getting started guide for details.\nOnce you have an account set up with your SUNet ID &lt;username&gt;, you can access Sherlock via any SSH client client. If you are using a UNIX-like system (e.g., MacOS) and you are using terminal to connect to sherlock, a useful resource is to set up an ssh config file. You can do this by editing or creating the file ~/.ssh/config, and adding the following lines:\n\n\n~/.ssh/config\n\nHost sherlock\n    HostName login.sherlock.stanford.edu\n    User &lt;username&gt;\n    KexAlgorithms +diffie-hellman-group-exchange-sha1\n\nNavigating to terminal, you can login to Sherlock using:\n$ ssh sherlock\nand then follow the remainder of the instructions Sherlock connection guide.\n\n\n\nThe Stanford filesystems have fixed allocations for individuals and groups. As such, it will be useful for you to be able to determine how much space you/the group have, so that you can optimally manage your resources. For extended details on storage with Sherlock, check out Sherlock storage guide.\nThere are several commands that we find extremely useful for working on Sherlock. We will go over several of them.\nSherlock has fixed allocations for the storage of individuals and groups. As such, you will be required to properly manage your storage allocations, re-allocating data to group-level directories as necessary.\nTo check your quotas for your group &lt;groupname&gt;, you can use the sh_quota command:\n$ sh_quota\n+---------------------------------------------------------------------------+\n| Disk usage for user &lt;username&gt; (group: &lt;groupname&gt;)                       |\n+---------------------------------------------------------------------------+\n|   Filesystem |  volume /   limit                  | inodes /  limit       |\n+---------------------------------------------------------------------------+\n          HOME |   9.4GB /  15.0GB [||||||     62%] |      - /      - (  -%)\n    GROUP_HOME | 562.6GB /   1.0TB [|||||      56%] |      - /      - (  -%)\n       SCRATCH |  65.0GB / 100.0TB [            0%] | 143.8K /  20.0M (  0%)\n GROUP_SCRATCH | 172.2GB / 100.0TB [            0%] |  53.4K /  20.0M (  0%)\n           OAK |  30.8TB / 240.0TB [|          12%] |   6.6M /  36.0M ( 18%)\n+---------------------------------------------------------------------------+\nsh_quota is a Sherlock-specific command that provides a general overview for all partitions a user has access to. Documentation is provided on their wiki. When your home directory begins to get filled, it may be valuable to consider moving files to scratch directories, or group directories. HOME, GROUP_HOME, and OAK are persistent storage; *SCRATCH directories are subject to purging.\nAnother useful tool is the disk usage command du. A useful and more interacrtive version of this command is ncdu. To use ncdu, add the following line to the bottom of your ~/.bash_profile, which will load the ncdu module each time you log in to Sherlock:\nml system ncdu\nand then update your ~/.bash_profile for the current session with:\n$ source ~/.bash_profile\nwhich will allow the current session to load the new module updated in your bash profile for ncdu. For future login sessions, re-sourcing your ~/.bash_profile is unnecessary.\nIn the present and future login sessions, you can access the ncdu command via:\n$ ncdu &lt;folder&gt;\nwhich will launch an interactive window for monitoring directory sizes from the folder specified by &lt;folder&gt;. Sherlock recommends running it in an interactive job. This can be useful when identifying where quota usage is being allocated.",
    "crumbs": [
      "Lab guide",
      "Computing",
      "Sherlock",
      "Access and resources"
    ]
  },
  {
    "objectID": "labguide/computing/sherlock/access-and-resources.html#access-and-resources",
    "href": "labguide/computing/sherlock/access-and-resources.html#access-and-resources",
    "title": "The Poldrack Lab \\@Stanford",
    "section": "",
    "text": "This section describes getting initial access to Sherlock and monitoring available resources.\n\n\n\n\nIf you are a new member of a lab at stanford, you will need to have your PI email Sherlock’s support to get your SUNet account configured for use with computing resources. See the Sherlock getting started guide for details.\nOnce you have an account set up with your SUNet ID &lt;username&gt;, you can access Sherlock via any SSH client client. If you are using a UNIX-like system (e.g., MacOS) and you are using terminal to connect to sherlock, a useful resource is to set up an ssh config file. You can do this by editing or creating the file ~/.ssh/config, and adding the following lines:\n\n\n~/.ssh/config\n\nHost sherlock\n    HostName login.sherlock.stanford.edu\n    User &lt;username&gt;\n    KexAlgorithms +diffie-hellman-group-exchange-sha1\n\nNavigating to terminal, you can login to Sherlock using:\n$ ssh sherlock\nand then follow the remainder of the instructions Sherlock connection guide.\n\n\n\nThe Stanford filesystems have fixed allocations for individuals and groups. As such, it will be useful for you to be able to determine how much space you/the group have, so that you can optimally manage your resources. For extended details on storage with Sherlock, check out Sherlock storage guide.\nThere are several commands that we find extremely useful for working on Sherlock. We will go over several of them.\nSherlock has fixed allocations for the storage of individuals and groups. As such, you will be required to properly manage your storage allocations, re-allocating data to group-level directories as necessary.\nTo check your quotas for your group &lt;groupname&gt;, you can use the sh_quota command:\n$ sh_quota\n+---------------------------------------------------------------------------+\n| Disk usage for user &lt;username&gt; (group: &lt;groupname&gt;)                       |\n+---------------------------------------------------------------------------+\n|   Filesystem |  volume /   limit                  | inodes /  limit       |\n+---------------------------------------------------------------------------+\n          HOME |   9.4GB /  15.0GB [||||||     62%] |      - /      - (  -%)\n    GROUP_HOME | 562.6GB /   1.0TB [|||||      56%] |      - /      - (  -%)\n       SCRATCH |  65.0GB / 100.0TB [            0%] | 143.8K /  20.0M (  0%)\n GROUP_SCRATCH | 172.2GB / 100.0TB [            0%] |  53.4K /  20.0M (  0%)\n           OAK |  30.8TB / 240.0TB [|          12%] |   6.6M /  36.0M ( 18%)\n+---------------------------------------------------------------------------+\nsh_quota is a Sherlock-specific command that provides a general overview for all partitions a user has access to. Documentation is provided on their wiki. When your home directory begins to get filled, it may be valuable to consider moving files to scratch directories, or group directories. HOME, GROUP_HOME, and OAK are persistent storage; *SCRATCH directories are subject to purging.\nAnother useful tool is the disk usage command du. A useful and more interacrtive version of this command is ncdu. To use ncdu, add the following line to the bottom of your ~/.bash_profile, which will load the ncdu module each time you log in to Sherlock:\nml system ncdu\nand then update your ~/.bash_profile for the current session with:\n$ source ~/.bash_profile\nwhich will allow the current session to load the new module updated in your bash profile for ncdu. For future login sessions, re-sourcing your ~/.bash_profile is unnecessary.\nIn the present and future login sessions, you can access the ncdu command via:\n$ ncdu &lt;folder&gt;\nwhich will launch an interactive window for monitoring directory sizes from the folder specified by &lt;folder&gt;. Sherlock recommends running it in an interactive job. This can be useful when identifying where quota usage is being allocated.",
    "crumbs": [
      "Lab guide",
      "Computing",
      "Sherlock",
      "Access and resources"
    ]
  },
  {
    "objectID": "labguide/publication/openaccess.html",
    "href": "labguide/publication/openaccess.html",
    "title": "Open access and preprints",
    "section": "",
    "text": "When we select journals for publication, we will prioritize publishing in journals that offer an Open Access option. However, we realize that sometimes the most appropriate journal for a particular paper will be one that does not offer this option.\nWhen project funding is not available for open access publication fees, Dr. Poldrack will cover these fees using unrestricted funds.",
    "crumbs": [
      "Lab guide",
      "Publication and dissemination",
      "Open access and preprints"
    ]
  },
  {
    "objectID": "labguide/publication/openaccess.html#open-access",
    "href": "labguide/publication/openaccess.html#open-access",
    "title": "Open access and preprints",
    "section": "",
    "text": "When we select journals for publication, we will prioritize publishing in journals that offer an Open Access option. However, we realize that sometimes the most appropriate journal for a particular paper will be one that does not offer this option.\nWhen project funding is not available for open access publication fees, Dr. Poldrack will cover these fees using unrestricted funds.",
    "crumbs": [
      "Lab guide",
      "Publication and dissemination",
      "Open access and preprints"
    ]
  },
  {
    "objectID": "labguide/publication/openaccess.html#preprints",
    "href": "labguide/publication/openaccess.html#preprints",
    "title": "Open access and preprints",
    "section": "Preprints",
    "text": "Preprints\n\nAll papers should be uploaded to a relevant preprint archive prior to or upon submission, unless the target journal does not allow preprint publication.\nOur general policy is to avoid publishing in journals that do not allow preprint posting, unless there is a very strong reason to do so.",
    "crumbs": [
      "Lab guide",
      "Publication and dissemination",
      "Open access and preprints"
    ]
  },
  {
    "objectID": "labguide/publication/addressing_errors.html",
    "href": "labguide/publication/addressing_errors.html",
    "title": "Addressing errors",
    "section": "",
    "text": "If we identify an error in a published paper from the lab, all authors should convene promptly to discuss what if any conclusions are affected by the error. Most journals have a mechanism for either publishing a correction to the article when the primary conclusions of the paper are not substantially affected, or retracting the article if they are. All authors should agree on whether a correction or retraction is warranted, and promptly notify the journal of their recommendation and the nature of the error. If applicable, a notification and corrected version of the article should also be submitted to the preprint server (e.g. bioRxiv), and internal copies of the paper should be corrected and versioned accordingly.",
    "crumbs": [
      "Lab guide",
      "Publication and dissemination",
      "Addressing errors"
    ]
  },
  {
    "objectID": "labguide/mentoring/mentoring.html",
    "href": "labguide/mentoring/mentoring.html",
    "title": "Training and Mentorship",
    "section": "",
    "text": "This section of the guide is a first-person perspective from Dr. Poldrack.\n\n\nWe are always looking for undergraduate students to get involved on a variety of projects! If you are a Stanford affiliate and in the area, feel free to fill out this form and we will reach out to you.\n\n\n\n\n\n\nI am affiliated with several graduate programs at Stanford:\n\nPsychology\nNeuroscience\nBioengineering\nBiomedical Data Science (BMI)\n\n\n\n\n\nIn choosing a program, there are several important differences:\n\nResearch: While most of these programs are fairly flexible, there are generally some expectations regarding the kind of research you will do, depending on the specific program. For example, if you are joining the BMI program then your work is expected to have at least some focus on novel data analysis or informatics methods, whereas if you are joining Psychology your work is expected to make some contact with psychological function. Having said that, most of what we do in our lab could be done by a student in any of these programs.\nCoursework: Perhaps the biggest difference between programs is the kind of courses you are required to take. Each program has a set of core requirements. In psychology, you will take a number of core courses in different areas of psychology (cognitive, neuroscience, social, affective, developmental). In the neuroscience program you will take a set of core modules spanning different areas of neuroscience (including one on cognitive neuroscience that Justin Gardner and I teach), whereas in BMI you take core courses around informatics-related topics. In each program you will also take elective courses (often outside the department) that establish complementary core knowledge that is important for your particular research; for example, you can take courses in our world-class statistics department regardless of which program you enroll in. One way to think about this is: What do you want to learn about that is outside of your specific content area? Take a look at the core courses in each program and see which ones interest you the most.\nFirst-year experience: In Psychology, students generally jump straight into a specific lab (or a collaboration between labs), and spend their first year doing a first-year project that they present to their area meeting at the end of the year. In Neuroscience and BMI, students do rotations in multiple labs in their first year, and are expected to pick a lab by the end of their first year.\nAdmissions: All of these programs are highly selective, but each differs in the nature of its admissions process. At one end of the spectrum is the Psychology admissions process, where initial decisions for who to interview are made by the combined faculty within each area of the department. At the other end is the Neuroscience program, where initial decisions are made by an admissions committee. As a generalization, I would say that the Psychology process is better for candidates whose interests and experience fit very closely with a specific PI or set of PIs, whereas the committee process caters towards candidates who may not have settled on a specific topic or PI.\nCareer positioning: I think that the specific department that one graduates from matters a lot less than people think it does. For example, I have been in psychology departments that have hired faculty with PhDs in physics, applied mathematics, and computer science. I think that the work that you do and the skills that you acquire ultimately matter a lot more than the name of the program that is listed on your diploma. Having said that, you ultimately do need to position yourself so that a particular department will feel that you “fit” with them; this is often a challenge for people working on interdisciplinary topics, where sometimes it can be difficult to find a department who feels that the person fits well.\n\n\n\n\nThere are always more qualified applicants than there are spots in our graduate programs, and there is no way to guarantee admission to any particular program. On the flipside, there are also no absolute requirements: we look at the whole picture, and other factors can sometimes outweigh a weaker academic record. There are a few factors that are particularly important for admission to my lab:\n\nResearch experience: It is very rare for someone to be accepted into any of the programs I am affiliated with at Stanford without significant research experience. Sometimes this can be obtained as an undergraduate, but more often successful applicants to our program have spent at least a year working as a research assistant in an active research laboratory. There are a couple of important reasons for this. First, we want you to understand what you are getting into; many people have rosy ideas of what it’s like to be a scientist, which can fall away pretty quickly in light of the actual experience of doing science. Spending some time in a lab helps you make sure that this is how you want to spend your life. In addition, it provides you with someone who can write a recommendation letter that speaks very directly to your potential as a researcher. Letters are a very important part of the admissions process, and the most effective letters are those that go into specific detail about your abilities, aptitude, and motivation.\nTechnical skills: The research that we do in our lab is highly technical, requiring knowledge of computing systems, programming, and math/statistics. I would say that decent programming ability is a pretty firm prerequisite for entering my lab; once you enter the lab I want you to be able to jump directly into doing science, and this just can’t happen if you have to spend a year teaching yourself how to program from scratch. More generally, we expect you to be able to pick up new technical topics easily; I don’t expect students to necessarily show up knowing how a reinforcement learning model works, but I expect them to be able to go and figure it out on their own by reading the relevant papers and then implement it on their own. The best way to demonstrate programming ability is to show a specific project that you have worked on. This could be an open source project that you have contributed to, or a project that you did on the side for fun (for example, mine your own social media feed, or program a cognitive task and measure how your own behavior changes from day to day). If you don’t currently know how to program, see my post on learning to program from scratch, and get going!\n\nRisk taking and resilience: If we are doing interesting science then things are going to fail, and we have to learn from those failures and move on.  I want to know that you are someone who is willing to go out on a limb to try something risky, and can handle the inevitable failures gracefully.  Rather than seeing a statement of purpose that only lists all of your successes, I find it very useful to also know about risks you have taken (be they physical, social, or emotional), challenges you have faced, failures you have experienced, and most importantly what you learned from all of these experiences.\n\n\n\n\n\nDifferent advisers have different philosophies, and it’s important to be sure that you pick an advisor whose style is right for you. I would say that the most important characteristic of my style is that I foster independent thinking in my trainees. Publishing papers is important, but not as important as developing one’s ability to conceive novel and interesting questions and ask them in a rigorous way. This means that beyond the first year project, I don’t generally hand my students problems to work on; rather, I expect them to come up with their own questions, and then we work together to devise the right experiments to test them. Another important thing to know is that I try to motivate by example, rather than by command. I rarely breathe down my trainees necks about getting their work done, because I work on the assumption that they will be self-motivated. On the other hand, I’m fairly hands-on in the sense that I still love to get deep in the weeds of experimental design and data analysis; for example, I will regularly code alongside lab members. I would also add that I am highly amenable to joint mentorship with other faculty.\n\n\n\nI have instituted a policy that I will no longer meet one-on-one with potential graduate students prior to the application process to discuss potential admission into my lab, as this has the potential to exacerbate existing disparities in graduate school admissions. I am willing to meet with individuals (particularly those from underrepresented groups) to discuss the graduate admissions process and other academic issues more generally, as time permits.\n\n\n\n\n\n\n\nWe do not generally advertise postdoctoral positions, so if you are interested in a position in the lab, contact Dr. Poldrack directly. Be sure to send your CV, and explain in brief your research interests and why you think you would be a good fit for the lab.\nWe are particularly interested in hiring postdocs who bring diversity to the lab, either through their personal identity, research interests, or experience.\n\n\n\n\n\n\n\nGraduate students in the lab are expected to keep suitable documentation for all research activities (i.e. a github log, a google doc, etc).\n\n\n\nDr. Poldrack provides all lab members with access to a scheduling system with which they can schedule meetings during any open times on his calendar. - Upon request, lab members can schedule a regular standing meeting.\n\n\n\n\nI view mentorship as a life-long relationship, and you can view working with me as having a “Lifetime Guarantee” in the sense that I will always be there to help you, even long after you have left the lab – from letters of recommendation to career mentorship to personal advice.",
    "crumbs": [
      "Lab guide",
      "Training and Mentorship"
    ]
  },
  {
    "objectID": "labguide/mentoring/mentoring.html#undergraduate-study",
    "href": "labguide/mentoring/mentoring.html#undergraduate-study",
    "title": "Training and Mentorship",
    "section": "",
    "text": "We are always looking for undergraduate students to get involved on a variety of projects! If you are a Stanford affiliate and in the area, feel free to fill out this form and we will reach out to you.",
    "crumbs": [
      "Lab guide",
      "Training and Mentorship"
    ]
  },
  {
    "objectID": "labguide/mentoring/mentoring.html#graduate-study",
    "href": "labguide/mentoring/mentoring.html#graduate-study",
    "title": "Training and Mentorship",
    "section": "",
    "text": "I am affiliated with several graduate programs at Stanford:\n\nPsychology\nNeuroscience\nBioengineering\nBiomedical Data Science (BMI)\n\n\n\n\n\nIn choosing a program, there are several important differences:\n\nResearch: While most of these programs are fairly flexible, there are generally some expectations regarding the kind of research you will do, depending on the specific program. For example, if you are joining the BMI program then your work is expected to have at least some focus on novel data analysis or informatics methods, whereas if you are joining Psychology your work is expected to make some contact with psychological function. Having said that, most of what we do in our lab could be done by a student in any of these programs.\nCoursework: Perhaps the biggest difference between programs is the kind of courses you are required to take. Each program has a set of core requirements. In psychology, you will take a number of core courses in different areas of psychology (cognitive, neuroscience, social, affective, developmental). In the neuroscience program you will take a set of core modules spanning different areas of neuroscience (including one on cognitive neuroscience that Justin Gardner and I teach), whereas in BMI you take core courses around informatics-related topics. In each program you will also take elective courses (often outside the department) that establish complementary core knowledge that is important for your particular research; for example, you can take courses in our world-class statistics department regardless of which program you enroll in. One way to think about this is: What do you want to learn about that is outside of your specific content area? Take a look at the core courses in each program and see which ones interest you the most.\nFirst-year experience: In Psychology, students generally jump straight into a specific lab (or a collaboration between labs), and spend their first year doing a first-year project that they present to their area meeting at the end of the year. In Neuroscience and BMI, students do rotations in multiple labs in their first year, and are expected to pick a lab by the end of their first year.\nAdmissions: All of these programs are highly selective, but each differs in the nature of its admissions process. At one end of the spectrum is the Psychology admissions process, where initial decisions for who to interview are made by the combined faculty within each area of the department. At the other end is the Neuroscience program, where initial decisions are made by an admissions committee. As a generalization, I would say that the Psychology process is better for candidates whose interests and experience fit very closely with a specific PI or set of PIs, whereas the committee process caters towards candidates who may not have settled on a specific topic or PI.\nCareer positioning: I think that the specific department that one graduates from matters a lot less than people think it does. For example, I have been in psychology departments that have hired faculty with PhDs in physics, applied mathematics, and computer science. I think that the work that you do and the skills that you acquire ultimately matter a lot more than the name of the program that is listed on your diploma. Having said that, you ultimately do need to position yourself so that a particular department will feel that you “fit” with them; this is often a challenge for people working on interdisciplinary topics, where sometimes it can be difficult to find a department who feels that the person fits well.\n\n\n\n\nThere are always more qualified applicants than there are spots in our graduate programs, and there is no way to guarantee admission to any particular program. On the flipside, there are also no absolute requirements: we look at the whole picture, and other factors can sometimes outweigh a weaker academic record. There are a few factors that are particularly important for admission to my lab:\n\nResearch experience: It is very rare for someone to be accepted into any of the programs I am affiliated with at Stanford without significant research experience. Sometimes this can be obtained as an undergraduate, but more often successful applicants to our program have spent at least a year working as a research assistant in an active research laboratory. There are a couple of important reasons for this. First, we want you to understand what you are getting into; many people have rosy ideas of what it’s like to be a scientist, which can fall away pretty quickly in light of the actual experience of doing science. Spending some time in a lab helps you make sure that this is how you want to spend your life. In addition, it provides you with someone who can write a recommendation letter that speaks very directly to your potential as a researcher. Letters are a very important part of the admissions process, and the most effective letters are those that go into specific detail about your abilities, aptitude, and motivation.\nTechnical skills: The research that we do in our lab is highly technical, requiring knowledge of computing systems, programming, and math/statistics. I would say that decent programming ability is a pretty firm prerequisite for entering my lab; once you enter the lab I want you to be able to jump directly into doing science, and this just can’t happen if you have to spend a year teaching yourself how to program from scratch. More generally, we expect you to be able to pick up new technical topics easily; I don’t expect students to necessarily show up knowing how a reinforcement learning model works, but I expect them to be able to go and figure it out on their own by reading the relevant papers and then implement it on their own. The best way to demonstrate programming ability is to show a specific project that you have worked on. This could be an open source project that you have contributed to, or a project that you did on the side for fun (for example, mine your own social media feed, or program a cognitive task and measure how your own behavior changes from day to day). If you don’t currently know how to program, see my post on learning to program from scratch, and get going!\n\nRisk taking and resilience: If we are doing interesting science then things are going to fail, and we have to learn from those failures and move on.  I want to know that you are someone who is willing to go out on a limb to try something risky, and can handle the inevitable failures gracefully.  Rather than seeing a statement of purpose that only lists all of your successes, I find it very useful to also know about risks you have taken (be they physical, social, or emotional), challenges you have faced, failures you have experienced, and most importantly what you learned from all of these experiences.\n\n\n\n\n\nDifferent advisers have different philosophies, and it’s important to be sure that you pick an advisor whose style is right for you. I would say that the most important characteristic of my style is that I foster independent thinking in my trainees. Publishing papers is important, but not as important as developing one’s ability to conceive novel and interesting questions and ask them in a rigorous way. This means that beyond the first year project, I don’t generally hand my students problems to work on; rather, I expect them to come up with their own questions, and then we work together to devise the right experiments to test them. Another important thing to know is that I try to motivate by example, rather than by command. I rarely breathe down my trainees necks about getting their work done, because I work on the assumption that they will be self-motivated. On the other hand, I’m fairly hands-on in the sense that I still love to get deep in the weeds of experimental design and data analysis; for example, I will regularly code alongside lab members. I would also add that I am highly amenable to joint mentorship with other faculty.\n\n\n\nI have instituted a policy that I will no longer meet one-on-one with potential graduate students prior to the application process to discuss potential admission into my lab, as this has the potential to exacerbate existing disparities in graduate school admissions. I am willing to meet with individuals (particularly those from underrepresented groups) to discuss the graduate admissions process and other academic issues more generally, as time permits.",
    "crumbs": [
      "Lab guide",
      "Training and Mentorship"
    ]
  },
  {
    "objectID": "labguide/mentoring/mentoring.html#postdoctoral-training-in-the-lab",
    "href": "labguide/mentoring/mentoring.html#postdoctoral-training-in-the-lab",
    "title": "Training and Mentorship",
    "section": "",
    "text": "We do not generally advertise postdoctoral positions, so if you are interested in a position in the lab, contact Dr. Poldrack directly. Be sure to send your CV, and explain in brief your research interests and why you think you would be a good fit for the lab.\nWe are particularly interested in hiring postdocs who bring diversity to the lab, either through their personal identity, research interests, or experience.",
    "crumbs": [
      "Lab guide",
      "Training and Mentorship"
    ]
  },
  {
    "objectID": "labguide/mentoring/mentoring.html#mentoring-practices",
    "href": "labguide/mentoring/mentoring.html#mentoring-practices",
    "title": "Training and Mentorship",
    "section": "",
    "text": "Graduate students in the lab are expected to keep suitable documentation for all research activities (i.e. a github log, a google doc, etc).\n\n\n\nDr. Poldrack provides all lab members with access to a scheduling system with which they can schedule meetings during any open times on his calendar. - Upon request, lab members can schedule a regular standing meeting.",
    "crumbs": [
      "Lab guide",
      "Training and Mentorship"
    ]
  },
  {
    "objectID": "labguide/mentoring/mentoring.html#lifetime-guarantee",
    "href": "labguide/mentoring/mentoring.html#lifetime-guarantee",
    "title": "Training and Mentorship",
    "section": "",
    "text": "I view mentorship as a life-long relationship, and you can view working with me as having a “Lifetime Guarantee” in the sense that I will always be there to help you, even long after you have left the lab – from letters of recommendation to career mentorship to personal advice.",
    "crumbs": [
      "Lab guide",
      "Training and Mentorship"
    ]
  },
  {
    "objectID": "labguide/general_lab_policies.html",
    "href": "labguide/general_lab_policies.html",
    "title": "General lab policies",
    "section": "",
    "text": "The laboratory has several administrative contacts:\nDiana Savastio is our departmental administrator. She is responsible for the following for individuals in the lab:\n\nTravel arrangements\nPurchasing\nReimbursement for purchases or travel\nVisitor Appointments\n\nSarah Reboli is Dr. Poldrack’s executive assistant. She is responsible for:\n\nMeeting scheduling and organization\nData use agreements\nIRB protocols\n\nJustin Dixon is the Psychology Department front office administrator. He is responsible for:\n\nRoom scheduling\nShipping/receiving",
    "crumbs": [
      "Lab guide",
      "General lab policies"
    ]
  },
  {
    "objectID": "labguide/general_lab_policies.html#administrative-requests",
    "href": "labguide/general_lab_policies.html#administrative-requests",
    "title": "General lab policies",
    "section": "",
    "text": "The laboratory has several administrative contacts:\nDiana Savastio is our departmental administrator. She is responsible for the following for individuals in the lab:\n\nTravel arrangements\nPurchasing\nReimbursement for purchases or travel\nVisitor Appointments\n\nSarah Reboli is Dr. Poldrack’s executive assistant. She is responsible for:\n\nMeeting scheduling and organization\nData use agreements\nIRB protocols\n\nJustin Dixon is the Psychology Department front office administrator. He is responsible for:\n\nRoom scheduling\nShipping/receiving",
    "crumbs": [
      "Lab guide",
      "General lab policies"
    ]
  },
  {
    "objectID": "labguide/general_lab_policies.html#lab-procedures",
    "href": "labguide/general_lab_policies.html#lab-procedures",
    "title": "General lab policies",
    "section": "Lab procedures",
    "text": "Lab procedures\nInternal information about lab procedures are available via a set of Google docs (all of which require permission to access):\n\nFinancial procedures\nTravel procedures\nOnboarding procedures for new lab members\nOffboarding procedures for departing lab members",
    "crumbs": [
      "Lab guide",
      "General lab policies"
    ]
  },
  {
    "objectID": "labguide/publication/conferences.html",
    "href": "labguide/publication/conferences.html",
    "title": "Conferences",
    "section": "",
    "text": "The laboratory will generally fund the attendance of members to at least one conference per year, pending funding availability.\n\nWhile funding is usually tied to presenting at the meeting, in some cases the lab will fund members to attend who are not presenting, depending on the meeting location and funding availability.\nWe regularly fund the attendance of research assistants to conferences as we think this is an essential aspect of their training experience.\n\nConference abstract submissions must be completed at least one week before the final deadline, to allow coauthors sufficient time to review the submission.\nLab members are encouraged to find satellite opportunities (such as hackathons or workshops) to maximize the benefit of their conference travel.\nConferences we attend (roughly in order of frequency):\n\nOrganization for Human Brain Mapping (our primary meeting)\nCognitive Computational Neuroscience\nSociety for Neuroscience\nPsychonomic Society\nNeuroeconomics\nFLUX\nCognitive Neuroscience Society (CNS)",
    "crumbs": [
      "Lab guide",
      "Publication and dissemination",
      "Conferences"
    ]
  },
  {
    "objectID": "labguide/publication/authorship.html",
    "href": "labguide/publication/authorship.html",
    "title": "Authorship",
    "section": "",
    "text": "The lab’s policy follows the IJCME recommendations on Defining the Role of Authors and Contributors, which state that authorship should be based on four criteria:\n\nSubstantial contributions to the conception or design of the work; or the acquisition, analysis, or interpretation of data for the work; AND\nDrafting the work or revising it critically for important intellectual content; AND\nFinal approval of the version to be published; AND\nAgreement to be accountable for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved.\n\nImportantly, we view substantial contributions to data acquisition or analysis alone to be sufficient for authorship on the primary project manuscript, as long criteria 2-4 are met. For this reason, research assistants in our lab often end up being coauthors on our publications.\nCriterion #1 also implies that Dr. Poldrack will not accept authorship on any work that he has not played a substantial role in either designing, analyzing, or interpreting. Please do not send completed manuscripts with a request for him to be a coauthor.\nThe sharing of data is not considered to be sufficient on its own to warrant authorship; instead, credit should be given through citation of a data descriptor and/or dataset DOI. However, authorship is appropriate in cases where the dataset owner provides substantial input into the research, or when required by the data use agreement.",
    "crumbs": [
      "Lab guide",
      "Publication and dissemination",
      "Authorship"
    ]
  },
  {
    "objectID": "labguide/publication/authorship.html#authorship-policies",
    "href": "labguide/publication/authorship.html#authorship-policies",
    "title": "Authorship",
    "section": "",
    "text": "The lab’s policy follows the IJCME recommendations on Defining the Role of Authors and Contributors, which state that authorship should be based on four criteria:\n\nSubstantial contributions to the conception or design of the work; or the acquisition, analysis, or interpretation of data for the work; AND\nDrafting the work or revising it critically for important intellectual content; AND\nFinal approval of the version to be published; AND\nAgreement to be accountable for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved.\n\nImportantly, we view substantial contributions to data acquisition or analysis alone to be sufficient for authorship on the primary project manuscript, as long criteria 2-4 are met. For this reason, research assistants in our lab often end up being coauthors on our publications.\nCriterion #1 also implies that Dr. Poldrack will not accept authorship on any work that he has not played a substantial role in either designing, analyzing, or interpreting. Please do not send completed manuscripts with a request for him to be a coauthor.\nThe sharing of data is not considered to be sufficient on its own to warrant authorship; instead, credit should be given through citation of a data descriptor and/or dataset DOI. However, authorship is appropriate in cases where the dataset owner provides substantial input into the research, or when required by the data use agreement.",
    "crumbs": [
      "Lab guide",
      "Publication and dissemination",
      "Authorship"
    ]
  },
  {
    "objectID": "labguide/publication/authorship.html#affiliation",
    "href": "labguide/publication/authorship.html#affiliation",
    "title": "Authorship",
    "section": "Affiliation",
    "text": "Affiliation\nThe listed affiliation for a publication should include any institutions where one was affiliated while completing the work.\n- Any work completed at Stanford should include the Stanford affiliation, even if the individual has left Stanford. - Work completed prior to joining Stanford but published after joining the lab does not need to include the Stanford affiliation, but it may be included (along with the institution where the work was completed) if work was done on the paper after joining the lab.",
    "crumbs": [
      "Lab guide",
      "Publication and dissemination",
      "Authorship"
    ]
  },
  {
    "objectID": "labguide/publication/science_communication.html",
    "href": "labguide/publication/science_communication.html",
    "title": "Science communication",
    "section": "",
    "text": "We affirm the importance and value of engaging with non-expert audiences, and support the efforts of lab members in service of science communication. We encourage lab members to work to develop their science communication skills and to seek out opportunities to engage with the public about science.\nIn some cases, laboratory findings may be of sufficient public interest to warrant a University press release. In these cases we will work with the University’s public relations team to develop a press release. In our interactions with the public relations team, we will express our intent to not exaggerate or oversimplify the findings.\nIf we are contacted by members of the press about our research, we will try to make ourselves available for comment to the extent possible. We will work to use accessible language while also communicating the nuances of our work. We may ask journalists to send us paraphrased versions of our quotes to check them for accuracy before the publication of any article, though we recognize that some journalists might not be willing to do so.",
    "crumbs": [
      "Lab guide",
      "Publication and dissemination",
      "Science communication"
    ]
  },
  {
    "objectID": "labguide/computing/sherlock/job-submission.html",
    "href": "labguide/computing/sherlock/job-submission.html",
    "title": "The Poldrack Lab \\@Stanford",
    "section": "",
    "text": "Important\n\n\n\nLogin nodes are shared among many users and therefore must not be used to run computationally intensive tasks. Those should be submitted to the scheduler which will dispatch them on compute nodes.\n\n\n\n\nYou can check available resources with the sh_part command:\n$ sh_part\n     QUEUE STA   FREE  TOTAL   FREE  TOTAL RESORC  OTHER MAXJOBTIME    CORES       NODE   GRES\n PARTITION TUS  CORES  CORES  NODES  NODES PENDNG PENDNG  DAY-HR:MN    /NODE     MEM-GB (COUNT)\n    normal   *    153   1792      0     84    23k    127    7-00:00    20-24    128-191 -\n    bigmem         29     88      0      2      0      8    1-00:00    32-56   512-3072 -\n       dev         31     40      0      2      0      0    0-02:00       20        128 -\n       gpu         47    172      0      8    116      1    7-00:00    20-24    191-256 gpu:4(S:0-1)(2),gpu:4(S:0)(6)\nThis will list out the compute resources available to you, so that you can determine the optimal resource to use.\n\n\n\nsh_dev sessions run on dedicated compute nodes, ensuring minimal wait times when you need to access a node for testing script, debug code or any kind of interactive work. sh_dev also provides X11 forwarding via the submission host (typically the login node you’re connected to) and can thus be used to run GUI applications.\nUsers can specify sh_dev calls with specific memory requests.\n\n\nIf you prefer to submit an existing job script or other executable as an interactive job, you can use the salloc command:\nsalloc script.sh\nsalloc will start a Slurm job and allocate resources for it, but it will not automatically connect you to the allocated node(s). It will only start a new shell on the same node you launched salloc from, and set up the appropriate $SLURM_* environment variables.\n\n\n\n\nMost large, long-running jobs on Sherlock should be submitted via the job scheduler.\nMost jobs can be submitted the the scheduler using the sbatch command. Sherlock provides documentation for how to generate sbatch submission scripts. There is also an experimental slurm-o-matic tool to help in generating these scripts interactively.\n\n\nBest practice before launching large, long-running jobs on Sherlock is to run a short test job to evaluate the time memory requirements. The basic idea is to run a small test job with minimal resource requirements—so the job will run quickly—then re-queue the job with optimized resource requests.\nJobs can be evaluated along three dimensions: memory, parallelization (i.e., number of nodes and CPUs), and run time. We briefly highlight why each axis is important below, as well as how to evaluate its requirements.\n\nMemory: Evaluating memory requirements of completed jobs is straightforward tools such as sacct (see ?@sec-acct). Requesting excessive memory and not using it will count against your FairShare score.\nParallelization: Evaluating how well the job performance scales with added CPUs can be done using seff (see Section 1.4.1). Requesting CPUs then not using them will still count against your FairShare score.\nRun Time: Requesting excess time for your jobs will not count against your FairShare score, but it will affect how quickly the scheduler allocates resources to your job.\n\nBelow, we provide more detailed resources (and example commands) for monitoring jobs.\n\n\n\n\n\nUsers can monitor the status of their currently running jobs on Sherlock with squeue:\nsqueue -u $USER\nThis command will monitor how long a job has been sitting on queue or actively running, depending on status.\n\n\n\nMore detailed information on running jobs can be found using clush, which can be loaded via the module system:\nml system py-clustershell\nclush -N -w @job:&lt;your Job ID&gt; top -b -n 1\nOn Sherlock, it provides an easy way to run commands on nodes your jobs are running on, and collect back information. In the above example, we use clush to execute the top command, which provides real-time monitoring of job resource usage. Many other uses are possible; for example,\nclush -w @j:&lt;your Job ID&gt; ps -u$USER -o%cpu,rss,cmd\nWill return the CPU and memory usage of all processes for a given job ID.\n\n\n\n\nOne application commonly run within Poldracklab is fMRIPrep. Since lab members will often submit many fMRIPrep jobs at the same time, it is best practice to submit these as an array job.\nWe provide an example fMRIPrep array job script below for running 5 individual subjects.\n\n\nsubmit_fmriprep_array.sh\n\n#!/bin/bash\n#\n#SBATCH --job-name=fMRIPrep\n#SBATCH --output=fmriprep.%j.out\n#SBATCH --time=1-00:00\n#SBATCH --cpus-per-task=16\n#SBATCH --mem-per-cpu=8GB\n#SBATCH --mail-user=&lt;your-email&gt;@stanford.edu\n#SBATCH --mail-type=FAIL\n#SBATCH --array=1-5\n#SBATCH -p russpold,owners,normal\n\n\n# Define directories\n\nDATADIR=&lt;/your/project/directory&gt;\nSCRATCH=&lt;/your/scratch/directory&gt;\nSIMGDIR=&lt;/your/project/directory/simgs&gt;\n\n# Begin work section\nsubj_list=(`find $DATADIR -maxdepth 1 -type d -name 'sub-*' -printf '%f\\n' | sort -n -ts -k2.1`)\nsub=\"${subj_list[$SLURM_ARRAY_TASK_ID]}\"\necho \"SUBJECT_ID: \" $sub\n\nsingularity run --cleanenv -B ${DATADIR}:/data:ro \\\n    -B ${SCRATCH}:/out \\\n    -B ${DATADIR}/license.txt:/license/license.txt:ro \\\n    ${SIMGDIR}/fmriprep-23-2-0.simg \\\n    /data /out participant \\\n        --participant-label ${sub} \\\n    --output-space MNI152NLin2009cAsym:res-2 \\\n    -w /out/workdir \\\n    --notrack \\\n        --fs-license-file /license/license.txt \n\n\n\n\n\nRegardless of whether your job was run in an interactive (i.e., using sh_dev) or non-interactive (i.e., using sbatch session), it is useful to evaluate how many resources they consumed after running:\n\n\nNominally, the fastest and easiest way to get a summary report, for a given job, is the “SLURM efficiency” tool, seff. This tool returns a simple, human-readable format report that includes both allocated as well as actually used resources (nodes, CPUs, memory, wall time).\nseff &lt;your Job ID&gt;\nGenerally speaking, seff reports can be used to determine how well (if at all) a job parallelizes, how much memory to request for future implementations of the job, and how much time to request. More granular reporting, however, is possible using the sacct command.\n\n\n\nMore rigorous resource analysis can be performed after a job has completed by using SLURM accounting, or sacct. Again, SLURM provides a rigorous documentation, including using --format= to define which columns to output and the various options that can constrain a query.\nsacct --user=$USER --start=2023-09-01 --end=2023-09-03 --format=jobid,jobname,partition,account,nnodes,ncpus,reqmem,maxrss,elapsed,totalcpu,state,reason",
    "crumbs": [
      "Lab guide",
      "Computing",
      "Sherlock",
      "Job submission on Sherlock"
    ]
  },
  {
    "objectID": "labguide/computing/sherlock/job-submission.html#job-submission-on-sherlock",
    "href": "labguide/computing/sherlock/job-submission.html#job-submission-on-sherlock",
    "title": "The Poldrack Lab \\@Stanford",
    "section": "",
    "text": "Important\n\n\n\nLogin nodes are shared among many users and therefore must not be used to run computationally intensive tasks. Those should be submitted to the scheduler which will dispatch them on compute nodes.\n\n\n\n\nYou can check available resources with the sh_part command:\n$ sh_part\n     QUEUE STA   FREE  TOTAL   FREE  TOTAL RESORC  OTHER MAXJOBTIME    CORES       NODE   GRES\n PARTITION TUS  CORES  CORES  NODES  NODES PENDNG PENDNG  DAY-HR:MN    /NODE     MEM-GB (COUNT)\n    normal   *    153   1792      0     84    23k    127    7-00:00    20-24    128-191 -\n    bigmem         29     88      0      2      0      8    1-00:00    32-56   512-3072 -\n       dev         31     40      0      2      0      0    0-02:00       20        128 -\n       gpu         47    172      0      8    116      1    7-00:00    20-24    191-256 gpu:4(S:0-1)(2),gpu:4(S:0)(6)\nThis will list out the compute resources available to you, so that you can determine the optimal resource to use.\n\n\n\nsh_dev sessions run on dedicated compute nodes, ensuring minimal wait times when you need to access a node for testing script, debug code or any kind of interactive work. sh_dev also provides X11 forwarding via the submission host (typically the login node you’re connected to) and can thus be used to run GUI applications.\nUsers can specify sh_dev calls with specific memory requests.\n\n\nIf you prefer to submit an existing job script or other executable as an interactive job, you can use the salloc command:\nsalloc script.sh\nsalloc will start a Slurm job and allocate resources for it, but it will not automatically connect you to the allocated node(s). It will only start a new shell on the same node you launched salloc from, and set up the appropriate $SLURM_* environment variables.\n\n\n\n\nMost large, long-running jobs on Sherlock should be submitted via the job scheduler.\nMost jobs can be submitted the the scheduler using the sbatch command. Sherlock provides documentation for how to generate sbatch submission scripts. There is also an experimental slurm-o-matic tool to help in generating these scripts interactively.\n\n\nBest practice before launching large, long-running jobs on Sherlock is to run a short test job to evaluate the time memory requirements. The basic idea is to run a small test job with minimal resource requirements—so the job will run quickly—then re-queue the job with optimized resource requests.\nJobs can be evaluated along three dimensions: memory, parallelization (i.e., number of nodes and CPUs), and run time. We briefly highlight why each axis is important below, as well as how to evaluate its requirements.\n\nMemory: Evaluating memory requirements of completed jobs is straightforward tools such as sacct (see ?@sec-acct). Requesting excessive memory and not using it will count against your FairShare score.\nParallelization: Evaluating how well the job performance scales with added CPUs can be done using seff (see Section 1.4.1). Requesting CPUs then not using them will still count against your FairShare score.\nRun Time: Requesting excess time for your jobs will not count against your FairShare score, but it will affect how quickly the scheduler allocates resources to your job.\n\nBelow, we provide more detailed resources (and example commands) for monitoring jobs.\n\n\n\n\n\nUsers can monitor the status of their currently running jobs on Sherlock with squeue:\nsqueue -u $USER\nThis command will monitor how long a job has been sitting on queue or actively running, depending on status.\n\n\n\nMore detailed information on running jobs can be found using clush, which can be loaded via the module system:\nml system py-clustershell\nclush -N -w @job:&lt;your Job ID&gt; top -b -n 1\nOn Sherlock, it provides an easy way to run commands on nodes your jobs are running on, and collect back information. In the above example, we use clush to execute the top command, which provides real-time monitoring of job resource usage. Many other uses are possible; for example,\nclush -w @j:&lt;your Job ID&gt; ps -u$USER -o%cpu,rss,cmd\nWill return the CPU and memory usage of all processes for a given job ID.\n\n\n\n\nOne application commonly run within Poldracklab is fMRIPrep. Since lab members will often submit many fMRIPrep jobs at the same time, it is best practice to submit these as an array job.\nWe provide an example fMRIPrep array job script below for running 5 individual subjects.\n\n\nsubmit_fmriprep_array.sh\n\n#!/bin/bash\n#\n#SBATCH --job-name=fMRIPrep\n#SBATCH --output=fmriprep.%j.out\n#SBATCH --time=1-00:00\n#SBATCH --cpus-per-task=16\n#SBATCH --mem-per-cpu=8GB\n#SBATCH --mail-user=&lt;your-email&gt;@stanford.edu\n#SBATCH --mail-type=FAIL\n#SBATCH --array=1-5\n#SBATCH -p russpold,owners,normal\n\n\n# Define directories\n\nDATADIR=&lt;/your/project/directory&gt;\nSCRATCH=&lt;/your/scratch/directory&gt;\nSIMGDIR=&lt;/your/project/directory/simgs&gt;\n\n# Begin work section\nsubj_list=(`find $DATADIR -maxdepth 1 -type d -name 'sub-*' -printf '%f\\n' | sort -n -ts -k2.1`)\nsub=\"${subj_list[$SLURM_ARRAY_TASK_ID]}\"\necho \"SUBJECT_ID: \" $sub\n\nsingularity run --cleanenv -B ${DATADIR}:/data:ro \\\n    -B ${SCRATCH}:/out \\\n    -B ${DATADIR}/license.txt:/license/license.txt:ro \\\n    ${SIMGDIR}/fmriprep-23-2-0.simg \\\n    /data /out participant \\\n        --participant-label ${sub} \\\n    --output-space MNI152NLin2009cAsym:res-2 \\\n    -w /out/workdir \\\n    --notrack \\\n        --fs-license-file /license/license.txt \n\n\n\n\n\nRegardless of whether your job was run in an interactive (i.e., using sh_dev) or non-interactive (i.e., using sbatch session), it is useful to evaluate how many resources they consumed after running:\n\n\nNominally, the fastest and easiest way to get a summary report, for a given job, is the “SLURM efficiency” tool, seff. This tool returns a simple, human-readable format report that includes both allocated as well as actually used resources (nodes, CPUs, memory, wall time).\nseff &lt;your Job ID&gt;\nGenerally speaking, seff reports can be used to determine how well (if at all) a job parallelizes, how much memory to request for future implementations of the job, and how much time to request. More granular reporting, however, is possible using the sacct command.\n\n\n\nMore rigorous resource analysis can be performed after a job has completed by using SLURM accounting, or sacct. Again, SLURM provides a rigorous documentation, including using --format= to define which columns to output and the various options that can constrain a query.\nsacct --user=$USER --start=2023-09-01 --end=2023-09-03 --format=jobid,jobname,partition,account,nnodes,ncpus,reqmem,maxrss,elapsed,totalcpu,state,reason",
    "crumbs": [
      "Lab guide",
      "Computing",
      "Sherlock",
      "Job submission on Sherlock"
    ]
  },
  {
    "objectID": "labguide/computing/etiquette.html",
    "href": "labguide/computing/etiquette.html",
    "title": "Computing",
    "section": "",
    "text": "Any servers managed by lab members for research purposes must meet the Stanford Minimum Security Standards.",
    "crumbs": [
      "Lab guide",
      "Computing",
      "Computing"
    ]
  },
  {
    "objectID": "labguide/computing/etiquette.html#computer-security",
    "href": "labguide/computing/etiquette.html#computer-security",
    "title": "Computing",
    "section": "",
    "text": "Any servers managed by lab members for research purposes must meet the Stanford Minimum Security Standards.",
    "crumbs": [
      "Lab guide",
      "Computing",
      "Computing"
    ]
  },
  {
    "objectID": "labguide/computing/etiquette.html#shared-computing-platforms",
    "href": "labguide/computing/etiquette.html#shared-computing-platforms",
    "title": "Computing",
    "section": "2 Shared computing platforms",
    "text": "2 Shared computing platforms\nWe use two primary shared computing platforms:\n\nStanford Research Computing Center\nTexas Advanced Computing Center\n\nThe decision regarding which platform to use will depend upon the specific needs of the project, and can be discussed with Dr. Poldrack or other experienced researchers in the lab.\n\n2.1 Shared computing etiquette\nLab members should follow good shared computing etiquette:\n\nNever run a computing job on the login node\n\nFor testing purposes, you can access a development node.\n\nKeep in mind that our allocation on these systems is limited.\nRun a small test job before starting a large job\nEnsure that your jobs are efficiently using the system resources - if you aren’t sure, then ask for help!\nOn systems where the lab has a specific allocation of cores (e.g. Sherlock), don’t use all of the cores at once unless having first discussed your needs at the lab meeting or via the lab mailing list or Slack.\nBe sure to set up your environment properly:\n\nAlways set your umask to allow group read/write; otherwise you may create files that can’t be modified by others in the group.\n\nShare your wisdom\n\nIf you encounter a problem that is not documented in the lab’s internal documentation, then add it to the documentation once you have solved it\nIf you encounter an error in the documentation, then flag it and/or fix it\nYou can refer to this website for more information on Sherlock, and if you have any further questions you can contact the Sherlock admins via email/Slack or meet during specific Office Hours as specifiedhere.\n\nEach TACC system has its own dedicated user guide that provides substantial information about how to use the system.\nUsers of shared systems should clean up directories as projects are completed, removing intermediate files that are not necessary to keep. The storage of redundant data should be avoided, except where required for backups.",
    "crumbs": [
      "Lab guide",
      "Computing",
      "Computing"
    ]
  },
  {
    "objectID": "labguide/research/code_management.html",
    "href": "labguide/research/code_management.html",
    "title": "Code management and sharing",
    "section": "",
    "text": "All code should be managed using a version control system, preferably git.\n\nThere are many resources online for learning git. A good starting point is Git for beginners: The definitive practical guide.\nGit questions can be addressed to the #git channel on the Poldracklab Slack.\n\nCode should be regularly pushed to a remote server, preferably Github.\n\nOur default is that all repositories for research projects will be public, unless there is a strong reason to keep it private.\n\nAny substantive code copied or adapted from another source (e.g. stackoverflow) should be attributed to the source (including the URL of the source).\n\nReferences to the copied or adapted sources should be added to the script files where they are written (preferably on the line before the source).\nCreate a notices.md following this template to reference larger portions of code that were copied or adapted to your project.\n\nCare must be taken to avoid sharing identifiable data, private passwords, AWS credentials, and other confidential information when using Github.\n\nAny private information should be stored in a separate text file that is read in by the code, and those files should be added to the .gitignore file so that they will not be checked in.\nAny accidental breach should be dealt with and reported to Dr. Poldrack and other relevant parties immediately.\n\n\n\n\n\n\nAll coding projects should be reviewed prior to paper submission.\n\n\n\n\n\nAll code should be made available via Github, upon submission of the preprint at the latest.\nAll shared code should be accompanied by an open source license.\n\nOur general preference is for permissive licenses such as MIT or Apache, but other licenses may be used (e.g. in collaborative projects that use GPL-style licenses).\nIf code is used from other projects, the licensing requirements for that code should be obeyed.\n\nA Github release should be generated for code related to any paper submission, containing the exact code used to implement all of the included analyses for the submitted version.\n\nThe Github repository should be linked to Zenodo in order to generate a DOI for each release, and this DOI should be included in the manuscript (rather than the Github link).",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Code management and sharing"
    ]
  },
  {
    "objectID": "labguide/research/code_management.html#code-management",
    "href": "labguide/research/code_management.html#code-management",
    "title": "Code management and sharing",
    "section": "",
    "text": "All code should be managed using a version control system, preferably git.\n\nThere are many resources online for learning git. A good starting point is Git for beginners: The definitive practical guide.\nGit questions can be addressed to the #git channel on the Poldracklab Slack.\n\nCode should be regularly pushed to a remote server, preferably Github.\n\nOur default is that all repositories for research projects will be public, unless there is a strong reason to keep it private.\n\nAny substantive code copied or adapted from another source (e.g. stackoverflow) should be attributed to the source (including the URL of the source).\n\nReferences to the copied or adapted sources should be added to the script files where they are written (preferably on the line before the source).\nCreate a notices.md following this template to reference larger portions of code that were copied or adapted to your project.\n\nCare must be taken to avoid sharing identifiable data, private passwords, AWS credentials, and other confidential information when using Github.\n\nAny private information should be stored in a separate text file that is read in by the code, and those files should be added to the .gitignore file so that they will not be checked in.\nAny accidental breach should be dealt with and reported to Dr. Poldrack and other relevant parties immediately.",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Code management and sharing"
    ]
  },
  {
    "objectID": "labguide/research/code_management.html#code-review",
    "href": "labguide/research/code_management.html#code-review",
    "title": "Code management and sharing",
    "section": "",
    "text": "All coding projects should be reviewed prior to paper submission.",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Code management and sharing"
    ]
  },
  {
    "objectID": "labguide/research/code_management.html#code-sharing",
    "href": "labguide/research/code_management.html#code-sharing",
    "title": "Code management and sharing",
    "section": "",
    "text": "All code should be made available via Github, upon submission of the preprint at the latest.\nAll shared code should be accompanied by an open source license.\n\nOur general preference is for permissive licenses such as MIT or Apache, but other licenses may be used (e.g. in collaborative projects that use GPL-style licenses).\nIf code is used from other projects, the licensing requirements for that code should be obeyed.\n\nA Github release should be generated for code related to any paper submission, containing the exact code used to implement all of the included analyses for the submitted version.\n\nThe Github repository should be linked to Zenodo in order to generate a DOI for each release, and this DOI should be included in the manuscript (rather than the Github link).",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Code management and sharing"
    ]
  },
  {
    "objectID": "labguide/research/human_subjects.html",
    "href": "labguide/research/human_subjects.html",
    "title": "Human subjects research",
    "section": "",
    "text": "Human subjects research\n\nAll lab members engaged in research must complete the relevant human subjects training through CITI. A detailed list of relevant training courses is provided to new members as part of the lab’s internal onboarding materials.\n\nIf unsure if you are doing human subjects research, you can use the NIH Decision Tool\nNo personal identifiers should ever be associated with data recorded in the laboratory. This includes the 18 identifiers specified by HIPAA, as well as any other identifiers that could potentially be used to re-identify an individual.\nAll brain imaging data should be defaced prior to analysis and sharing. Behavioural data should not contain personal identifiers (such as for example Amazon Turk ID for data collected online).\n\nDemographic data must be obtained for all subjects, in order to fulfill NIH inclusion reporting requirements and IRB documentation.\nRedcap should be used for recruitment and screening, as well as e-consents.\n\nIRB maintenance is done by the research coordinators or the person leading the project.\n\nAll projects considered a clinical trial should be posted on ClinicalTrials.gov\nFor more information on ethics regarding human subject research, you can watch Stanford’s MED 255 videos on Biomedical Ethics",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Human subjects research"
    ]
  },
  {
    "objectID": "labguide/research/research.html",
    "href": "labguide/research/research.html",
    "title": "Research practices",
    "section": "",
    "text": "Research practices"
  },
  {
    "objectID": "labguide/research/intellectual_property.html",
    "href": "labguide/research/intellectual_property.html",
    "title": "Intellectual property",
    "section": "",
    "text": "All products of research at Stanford (including data and code) are the property of the University. However, faculty have wide latitude to release software as open source under the University’s open source policy, as long as it doesn’t conflict with any other obligations. Students are allowed to release code under an open source license at Stanford with faculty permission; students in the Poldracklab have blanket permission to do so, as all of our code is intended to be made open available, as discussed in the section on code sharing. After leaving the lab, trainees can continue to reuse any code or other research materials (e.g. stimuli) developed as part of their work in the lab as long as the code has been released under an open source license and they continue to abide by the terms of the license.\nAs discussed in the section on data management and sharing, all data collected within our laboratory is meant to be shared upon submission of the related paper. In cases where these data can be deidentified they will be shared under a public domain dedication (CC0), which places no restrictions on their use by other researchers. Thus, any researcher can continue to use those data once they leave the lab. In other cases it may be necessary to restrict data sharing (e.g. when the data cannot be deidentified), in which case researchers will need to obtain a data use agreement from Stanford in order to access those data at their new institution. Dr. Poldrack commits to supporting any such requests, unless they violate other obligations of his or the University.\nIn many cases we use data within the lab under Data Use Agreements (such as the ABCD or HCP datasets), and any researcher wishing to use such datasets must be explicitly listed on the DUA. Once a researcher is no longer included in the Stanford DUA for a particular dataset, they must no longer access the dataset via Stanford computer systems.\n\n\nWhen a trainee leaves the lab, there is often uncertainty about which ideas are theirs to pursue independently and which must be pursued in collaboration with their former mentor. It is difficult to specify a blanket policy on this issue, as it will depend on many different circumstances. Trainees are encouraged to discuss this issue openly with Dr. Poldrack prior to departing the lab, to prevent any misunderstandings. In many cases Dr. Poldrack will be happy for the trainee to take the ideas and pursue them independently in their new position; this has happened on a number of occasions. In other cases, particularly when the ideas were developed in close collaboration with Dr. Poldrack, there may be an expectation that further development of the ideas will occur through collaborative efforts.",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Intellectual property"
    ]
  },
  {
    "objectID": "labguide/research/intellectual_property.html#research-ideas",
    "href": "labguide/research/intellectual_property.html#research-ideas",
    "title": "Intellectual property",
    "section": "",
    "text": "When a trainee leaves the lab, there is often uncertainty about which ideas are theirs to pursue independently and which must be pursued in collaboration with their former mentor. It is difficult to specify a blanket policy on this issue, as it will depend on many different circumstances. Trainees are encouraged to discuss this issue openly with Dr. Poldrack prior to departing the lab, to prevent any misunderstandings. In many cases Dr. Poldrack will be happy for the trainee to take the ideas and pursue them independently in their new position; this has happened on a number of occasions. In other cases, particularly when the ideas were developed in close collaboration with Dr. Poldrack, there may be an expectation that further development of the ideas will occur through collaborative efforts.",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Intellectual property"
    ]
  },
  {
    "objectID": "labguide/research/preregistration.html",
    "href": "labguide/research/preregistration.html",
    "title": "Pre-registration and Registered Reports",
    "section": "",
    "text": "Pre-registration is a mechanism in which one places a detailed description of a planned study into a repository where it is timestamped and will remain immutable.\nFor more background information, see here.\n\nAll empirical projects will be pre-registered, preferably using the Open Science Framework.\n\nPilot studies need not be pre-registered, but should then be subject to a pre-registered replication or extension.\n\nPre-registration should include as much detail as possible. Pre-registration must include at minimum:\n\nSample size (with justification)\nInclusion/exclusion criteria\nCriteria and procedures for outlier exclusion and data transformation\nPrimary hypotheses or outcomes to be tested (or an explicit statement that the study is simply exploratory)\n\nIn cases where you can’t say exactly how you will do something (usually because the choices are data-dependent), outline your procedures for determining how to do the thing, and in particular what aspects of the data will go into making that decision.\n\nIn some cases, multi-stage pre-registration may be appropriate\n\nE.g. when an initial discovery sample is used to determine hypotheses for subsequent testing in a validation sample\nIn this case, the initial pre-registration should lay out the sampling plan and procedures for data splitting, as well as the plan for followup pre-registrations.\n\nFor fMRI studies, pre-registration should specify:\n\nAny anatomical regions of interest to be used (with a specific definition and/or image mask for the region)\nMotion modeling strategies (including trial- or subject-level exclusion criteria)\nConfound modeling strategies at the trial-, subject- and group-level (including the specific design of response time modeling strategies)\n\nDeviations from pre-registration\n\nPre-registration should not be viewed as handcuffs. If a detail of the pre-registration is clearly suboptimal, then the rationale for using a more appropriate method should be noted, and the optimal method should be used.\nPublications should include an explicit “Deviations from pre-registration” section that outlines any deviations and their rationale.\n\n\n\n\nWhenever possible, we support the submission of Registered Reports.\nRegistered reports are a publication mechanism in which one submits a manuscript that contains an Introduction and Methods section that describe a study prior to execution of the research; it is essential a written version of a pre-registration.\nThe manuscript is reviewed on the strength of the rationale and methods, and if deemed sufficient it is given an “in-principle acceptance” such that the journal guarantees to publish the paper regardless of the results, as long as the approved methods were followed.\nThere are currently over 300 journals that accept Registered Report submissions.\nIn addition, Registered Reports can be submitted to PCI-RR, which is a community project that reviews preprint Registered Reports.",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Pre-registration and Registered Reports"
    ]
  },
  {
    "objectID": "labguide/research/preregistration.html#pre-registration",
    "href": "labguide/research/preregistration.html#pre-registration",
    "title": "Pre-registration and Registered Reports",
    "section": "",
    "text": "Pre-registration is a mechanism in which one places a detailed description of a planned study into a repository where it is timestamped and will remain immutable.\nFor more background information, see here.\n\nAll empirical projects will be pre-registered, preferably using the Open Science Framework.\n\nPilot studies need not be pre-registered, but should then be subject to a pre-registered replication or extension.\n\nPre-registration should include as much detail as possible. Pre-registration must include at minimum:\n\nSample size (with justification)\nInclusion/exclusion criteria\nCriteria and procedures for outlier exclusion and data transformation\nPrimary hypotheses or outcomes to be tested (or an explicit statement that the study is simply exploratory)\n\nIn cases where you can’t say exactly how you will do something (usually because the choices are data-dependent), outline your procedures for determining how to do the thing, and in particular what aspects of the data will go into making that decision.\n\nIn some cases, multi-stage pre-registration may be appropriate\n\nE.g. when an initial discovery sample is used to determine hypotheses for subsequent testing in a validation sample\nIn this case, the initial pre-registration should lay out the sampling plan and procedures for data splitting, as well as the plan for followup pre-registrations.\n\nFor fMRI studies, pre-registration should specify:\n\nAny anatomical regions of interest to be used (with a specific definition and/or image mask for the region)\nMotion modeling strategies (including trial- or subject-level exclusion criteria)\nConfound modeling strategies at the trial-, subject- and group-level (including the specific design of response time modeling strategies)\n\nDeviations from pre-registration\n\nPre-registration should not be viewed as handcuffs. If a detail of the pre-registration is clearly suboptimal, then the rationale for using a more appropriate method should be noted, and the optimal method should be used.\nPublications should include an explicit “Deviations from pre-registration” section that outlines any deviations and their rationale.\n\n\n\n\nWhenever possible, we support the submission of Registered Reports.\nRegistered reports are a publication mechanism in which one submits a manuscript that contains an Introduction and Methods section that describe a study prior to execution of the research; it is essential a written version of a pre-registration.\nThe manuscript is reviewed on the strength of the rationale and methods, and if deemed sufficient it is given an “in-principle acceptance” such that the journal guarantees to publish the paper regardless of the results, as long as the approved methods were followed.\nThere are currently over 300 journals that accept Registered Report submissions.\nIn addition, Registered Reports can be submitted to PCI-RR, which is a community project that reviews preprint Registered Reports.",
    "crumbs": [
      "Lab guide",
      "Research practices",
      "Pre-registration and Registered Reports"
    ]
  },
  {
    "objectID": "labguide/environment/lab_environment.html",
    "href": "labguide/environment/lab_environment.html",
    "title": "Lab environment",
    "section": "",
    "text": "Lab environment\nIn the Poldrack Lab, we want all of our lab members to feel valued, appreciated, safe, and free to be who they are. We embrace diversity and are committed to preventing discrimination against people based on gender identity or expression, sexual orientation, race, ethnicity, religion, age, neurodiversity, disability status, citizenship, or any other feature that makes them unique."
  },
  {
    "objectID": "labguide/environment/work_life_balance.html",
    "href": "labguide/environment/work_life_balance.html",
    "title": "Work/life balance",
    "section": "",
    "text": "We aim to promote sustainable creativity and productivity for all of our members, which can’t be achieved without a healthy balance between work and personal life.\nNo lab member is expected to work in ways that interfere with a healthy life. Work should not be so excessive as to interfere with sleep, exercise, or family/personal time.\nLab members are expected to be proactive regarding deadlines, to avoid last-minute scrambles. Dr. Poldrack and other lab members may decline to support submissions that are prepared with insufficient lead time.\nWe aim to promote an environment that is understanding and supportive of individuals with child care and adult care responsibilities. This includes but is not limited to offering flexible work hours to help individuals with familial responsibilities.\n\nThe lab is committed to fully supporting individuals with chronic physical and/or mental illnesses, as well as individuals with accessibility needs. No lab member will be discriminated against because of their chronic diagnosis or health-related limitations of their ability to work. Lab members are encouraged to prioritize their physical and mental health as much as possible, and the lab is prepared to support them in doing so.\nLab members who are seeking time off for health (including mental health) reasons are strongly encouraged, though absolutely not required, to speak with Dr. Poldrack about their concerns. Dr. Poldrack commits to approaching these conversations in a nonjudgmental and empathetic manner, and to working with all lab members to find a solution that works best for them.",
    "crumbs": [
      "Lab guide",
      "Lab environment",
      "Work/life balance"
    ]
  },
  {
    "objectID": "labguide/environment/working_remotely.html",
    "href": "labguide/environment/working_remotely.html",
    "title": "Working remotely",
    "section": "",
    "text": "The Poldracklab has a long history of remote members, and we support the needs of lab members who must work remotely.\n\nAny remote work must be discussed with and approved by Dr. Poldrack and arranged in accordance with Stanford University Policies on Staff Telecommuting and Remote Working.",
    "crumbs": [
      "Lab guide",
      "Lab environment",
      "Working remotely"
    ]
  },
  {
    "objectID": "labguide/index.html",
    "href": "labguide/index.html",
    "title": "Lab guide",
    "section": "",
    "text": "This guide provides details about the mission, values, and practices of the Poldrack Lab at Stanford. This guide is intended for:\n\nIndividuals who are joining the lab, to describe the standards and practices that they are expected to adhere to\nIndividuals who are considering joining the lab, to provide a picture of the lab environment and practices.\n\nAdditional contributions to this guide are welcome!\n\nIf you wish to comment on the content or make suggestions, please use the issues page.\nIf you wish to edit the content directly, please start by discussing your proposed change on the issues page. Once you have made sure that your proposed change doesn’t overlap with other ongoing work, you can fork the repository and submit a pull request with your proposed change. See this guide from the Turing Way for details on how to contribute via pull request.\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "Poldracklab members",
    "section": "",
    "text": "PI\n\n  \n  Russ Poldrack  \n  \n\n\nResearch Scientists\n\n  \n  Patrick Bissett \n  \n  Joshua Buckholtz \n  \n  Michael Demidenko  \n  \n  Chris Markiewicz  \n  \n  Jeanette Mumford  \n  \n\n\nPostdoctoral Scholars\n\n  \n  Eric Bridgeford \n  \n  Anita Jwa \n  \n\n\nGraduate Students\n\n  \n  Lynde Folsom \n  \n  Nastasia Klevak \n  \n  Jocelyn Ricard \n  \n  Gustavo Santiago-Reyes \n  \n  Anna Xu \n  \n\n\nResearch Staff\n\n  \n  Kriti Achyutuni \n  \n  Logan Bennett \n  \n  Margot Mitchell \n  \n  Joe Wexler\n  \n\n\nSoftware Developers\n\n  \n  Ross Blair\n  \n  Mathias Goncalves  \n  \n  Nell Hardcastle  \n  \n  Greg Noack\n  \n\n\nLab Alumni\n\n  \n  Adam Aron\n  \n  Akram Bakkour\n  \n  Austin Brotman\n  \n  Mei-Yen Chen\n  \n  Naomi Cherne\n  \n  Rastko Ciric\n  \n  Jessica Cohen\n  \n  Eliza Congdon\n  \n  Tyler Davis\n  \n  Elizabeth DuPre\n  \n  Joke Durnez\n  \n  Ian Eisenberg\n  \n  Ayse Zeynep Enkavi\n  \n  Oscar Esteban\n  \n  Karin Foerde\n  \n  Adriana Galvan\n  \n  Marta Garrido\n  \n  Dara Ghahremani\n  \n  Chris Gorgolewski\n  \n  Grace Huckins\n  \n  Paul Jaffe\n  \n  Koji Jimura\n  \n  Don Kalar\n  \n  Sanmi Koyejo\n  \n  Agatha Lenartowicz\n  \n  Romy Lorenz\n  \n  Nick Malecek\n  \n  Dana Mastrovito\n  \n  Craig Moodie\n  \n  Martin Norgaard  \n  \n  Rajeev Raizada\n  \n  Jaime Rios\n  \n  Angela Rizk-Jackson\n  \n  Ajay Satpute\n  \n  Tom Schonberg\n  \n  Sunjae Shim\n  \n  Mac Shine\n  \n  Vanessa Sochat\n  \n  Elena Stover\n  \n  Armin Thomas\n  \n  William Thompson\n  \n  Christopher Trepel\n  \n  Matilde Vaghi\n  \n  Jonathan Walters\n  \n  Corey White\n  \n  Eliott Wimmer\n  \n  Alara Wright\n  \n  Gui Xue\n  \n\n\n\nNo matching items",
    "crumbs": [
      "Lab members"
    ]
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Research Projects",
    "section": "",
    "text": "A project that is characterizing the large-scale brain networks involved in diverse cognitive control processes using deep phenotyping.\n\nPrevious funding: 5R01MH117772\n\nA project that is reframing the Cognitive Systems component of RDoC using deep phenotyping with behavior and fMRI.\n\nCurrent funding: 1R01MH130898\n\nRepresentative publications:\n\nA dual-task approach to inform the taxomony of inhibition-related processes\n\n\n\n\n\n\nA project that is developing end-to-end neural network models for the dynamics of human performance on cognitive control tasks\nRepresentative publications:\n\nModelling human behaviour in cognitive tasks with latent dynamical systems\nAn image-computable model of speeded decision-making\n\n\n\n\n\n\nA project aimed at development next-generation models of response inhibition\nRepresentative publications:\n\nSevere violations of independence in response inhibition tasks\nEstimating the time to do nothing: Towards next-generation models of response inhibition.\nDesign issues and solutions for stop-signal data from the Adolescent Brain Cognitive Development (ABCD) study",
    "crumbs": [
      "Projects"
    ]
  },
  {
    "objectID": "projects.html#cognitive-neuroscience-projects",
    "href": "projects.html#cognitive-neuroscience-projects",
    "title": "Research Projects",
    "section": "",
    "text": "A project that is characterizing the large-scale brain networks involved in diverse cognitive control processes using deep phenotyping.\n\nPrevious funding: 5R01MH117772\n\nA project that is reframing the Cognitive Systems component of RDoC using deep phenotyping with behavior and fMRI.\n\nCurrent funding: 1R01MH130898\n\nRepresentative publications:\n\nA dual-task approach to inform the taxomony of inhibition-related processes\n\n\n\n\n\n\nA project that is developing end-to-end neural network models for the dynamics of human performance on cognitive control tasks\nRepresentative publications:\n\nModelling human behaviour in cognitive tasks with latent dynamical systems\nAn image-computable model of speeded decision-making\n\n\n\n\n\n\nA project aimed at development next-generation models of response inhibition\nRepresentative publications:\n\nSevere violations of independence in response inhibition tasks\nEstimating the time to do nothing: Towards next-generation models of response inhibition.\nDesign issues and solutions for stop-signal data from the Adolescent Brain Cognitive Development (ABCD) study",
    "crumbs": [
      "Projects"
    ]
  },
  {
    "objectID": "projects.html#informaticsanalysis-projects",
    "href": "projects.html#informaticsanalysis-projects",
    "title": "Research Projects",
    "section": "Informatics/analysis projects",
    "text": "Informatics/analysis projects\n\nOpenNeuro\n\nA data sharing archive for neuroscience data\nCurrent funding: R24MH117179\nRepresentative publications:\n\nThe OpenNeuro resource for sharing of neuroscience data\n\n\n\n\nThe impact of AI on Neuroimaging Data Privacy\n\nA project investigating the effectiveness of deidentification in the context of AI face recognition systems\nPrevious funding: Stanford Human-Centered Artificial Intelligence Institute\nRepresentative publications:\n\nDemystifying the likelihood of reidentification in neuroimaging data: A technical and regulatory analysis\n\n\n\n\nBrain Imaging Data Structure (BIDS)\n\nA framework for organization of neuroimaging data and metadata\nPrevious funding: R01MH126699, R24MH114705\nRepresentative publications:\n\nThe brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments\nThe past, present, and future of the brain imaging data structure (BIDS)\n\n\n\n\nExperiment Factory\n\nA platform for online behavioral experimentation\nCurrent funding: Columbia University Science of Behavior Change Resource and Coordinating Center\nRepresentative publications:\n\nThe Experiment Factory: Standardizing Behavioral Experiments\n\n\n\n\nNeurosynth\n\nA platform for coordinate-based meta-analysis on neuroimaging data\nPrevious funding: 5R01MH096906\nRepresentative publications:\n\nLarge-scale automated synthesis of human functional neuroimaging data\n\n\n\n\nfMRIPrep\n\nA robust and reproducible preprocessing workflow for fMRI data\nCurrent funding RF1MH121867\nRepresentative publications:\n\nfMRIPrep: a robust preprocessing pipeline for functional MRI\n\n\n\n\nWomen’s Brain Health Initiative Data Coordinating Center\n\nA project to coordinate data from several University of California Imaging Centers in service of better understanding women’s brain health.\nCurrent funding: Noyce Foundation\n\n\n\nCognitive Atlas\n\nAn ontology for cognitive science\nPrevious funding: 5R01MH082795\nRepresentative publications:\n\nThe cognitive atlas: toward a knowledge foundation for cognitive neuroscience\n\n\n\n\nNeurovault\n\nA data sharing archive for summary data from neuroimaging studies\nRepresentative publications:\n\nNeuroVault.org: a web-based repository for collecting and sharing unthresholded statistical maps of the human brain\n\n\n\n\nMRIQC\n\nA quality control tool for structural and functional MRI data\nRepresentative publications:\n\nMRIQC: Advancing the automatic prediction of image quality in MRI from unseen sites\n\n\n\n\nTemplateflow\n\nA FAIR sharing archive and toolkit for neuroimaging templates and atlases\nRepresentative publications:\n\nTemplateFlow: FAIR-sharing of multi-scale, multi-species brain models",
    "crumbs": [
      "Projects"
    ]
  },
  {
    "objectID": "research_subjects.html",
    "href": "research_subjects.html",
    "title": "Poldracklab - Volunteer to participate in research",
    "section": "",
    "text": "Please note, these project are active as of June 2023. If you take our screener, we’ll be sure to contact you as soon as possible if you’re eligible.\n\n\nWe currently have the following opportunities available for research participants:\n\n\nJoin us as a participant!\n\n\nThe Poldrack Lab is recruiting volunteers for studies of how learning, decision making, and executive function work in the brain.  These studies involve 12 sessions lasting from 1.5 - 2 hours each. During each session, you will be scanned using functional magnetic resonance imaging and/or perform computerized or paper and pencil behavioral tests.\n\n\nEligible participants must be between 18 and 50 years old, speak English fluently, be right-handed, free of psychiatric and neurological illness, have no metallic implants in or on their body, have no history of eating disorder, and have no food allergies.\n\n\nYou may receive a total of $630 as payment for your participation, which is $15/hr for all sessions, and a $210 completion bonus for completing all 12 scanning sessions and post-scan tasks, and $150 for completing the behavioral tasks.\n\n\nFor general information about participants rights, contact 1-866-680-2906.\n\n\nIf you are interested, please fill out the recruitment form here https://redcap.stanford.edu/surveys/?s=WYFLTRCFA8.\n\n\n\nIf you are a Stanford undergraduate interested in volunteering as a research assistant in the Poldrack Lab, you can sign up here.  Please note that we do not accept minors as volunteers in the laboratory."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Poldrack Lab \\@Stanford",
    "section": "",
    "text": "Our lab is focused on understanding how the brain gives rise to adaptive human behaviors. We use a combination of functional neuroimaging, cognitive psychology, and computational modeling to address this question. We also develop tools and infrastructure to make neuroscience more transparent and reproducible.\nIn the Poldrack Lab, we want all of our lab members to feel valued, appreciated, safe, and free to be who they are. We embrace diversity and are committed to preventing discrimination against people based on gender identity or expression, sexual orientation, race, ethnicity, religion, age, neurodiversity, disability status, citizenship, or any other feature that makes them unique.\nWe have an online Lab Guide that outlines our lab’s values, principles, and practices.\nLearn more about our lab’s work:\n\nProjects\nPublications\nPresentations\nSoftware\n\nInterested in participating in a research subject? See our listing of current studies.\nInterested in joining the lab? See our listing of open positions.\n\nThis work is licensed under a Creative Commons Attribution 4.0 International License.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Poldracklab Software and Data",
    "section": "",
    "text": "Our lab endeavors to make all of our research code openly available. None of this code is guaranteed to work outside of our local environment, and most of it is not built for portability, but if you use it and find any bugs, please let us know. Unless otherwise noted, all code is released under the unrestrictive MIT License\nFor additional information on our organized software development projects, see Informatics projects.",
    "crumbs": [
      "Code/Data"
    ]
  },
  {
    "objectID": "software.html#github-repositories",
    "href": "software.html#github-repositories",
    "title": "Poldracklab Software and Data",
    "section": "Github repositories",
    "text": "Github repositories\nThe Poldrack lab manages several GitHub organizations:\n\nPoldrack Lab at Stanford\nOpenNeuro.org\nThe Experiment Factory\nThe Cognitive Atlas\n\nWe are currently or have been heavily involved in the following organizations:\n\nBrain Imaging Data Structure\nNeuroVault\nNiPreps\nTemplateFlow\n\nWe also contribute to greater and lesser extents to many other projects, including:\n\nThe Nipy community\nNipype\nNilearn\n\nFor individual members’ GitHub profiles, see Lab members.",
    "crumbs": [
      "Code/Data"
    ]
  },
  {
    "objectID": "software.html#software-for-tasks-used-in-lab-publications",
    "href": "software.html#software-for-tasks-used-in-lab-publications",
    "title": "Poldracklab Software and Data",
    "section": "Software for tasks used in lab publications",
    "text": "Software for tasks used in lab publications\n\nMany of our recent tasks are housed in the Experiment Factory\nmixed gambles task (demo and scanning script from task used in Tom et al., 2007)",
    "crumbs": [
      "Code/Data"
    ]
  },
  {
    "objectID": "software.html#task-software-used-in-consortium-for-neuropsychiatric-phenomics-cnp",
    "href": "software.html#task-software-used-in-consortium-for-neuropsychiatric-phenomics-cnp",
    "title": "Poldracklab Software and Data",
    "section": "Task software used in Consortium for Neuropsychiatric Phenomics (CNP)",
    "text": "Task software used in Consortium for Neuropsychiatric Phenomics (CNP)\n\n​Users of these tasks should include the following acknowledgment in any resulting publications: Development of this software was supported by the Consortium for Neuropsychiatric Phenomics (NIH Roadmap for Medical Research grants UL1-DE019580, RL1MH083269, RL1DA024853, PL1MH083271.\nBalloon Analog Risk task (demo and scanning script)\nBreath holding task\nPaired Associate Memory\nSpatial working memory capacity\nStop Signal task\nTask switching",
    "crumbs": [
      "Code/Data"
    ]
  },
  {
    "objectID": "software.html#datacodeosf-pages-for-published-papers",
    "href": "software.html#datacodeosf-pages-for-published-papers",
    "title": "Poldracklab Software and Data",
    "section": "Data/code/OSF pages for published papers",
    "text": "Data/code/OSF pages for published papers\n\nCreating config file at /home/runner/.config/pybliometrics.cfg with default paths…",
    "crumbs": [
      "Code/Data"
    ]
  }
]